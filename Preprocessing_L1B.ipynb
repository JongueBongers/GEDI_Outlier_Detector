{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43779e86e13e3b84",
   "metadata": {},
   "source": [
    "# Preprocessing L1B\n",
    "We will extract all relevant datasets from all HDF5 files and convert them into a readable Pandas dataframe. We will also do some preliminary data cleaning and PCA analysis. Everything will be outputted as *.parquet* files. This dramatically reduces the memory usage in the outlier detector scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:12.404923Z",
     "start_time": "2024-12-03T16:42:12.088600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "# import geoviews as gv\n",
    "# from geoviews import opts, tile_sources as gvts\n",
    "# import holoviews as hv\n",
    "# gv.extension('bokeh', 'matplotlib')\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# import PyQt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abfb1225ff906d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:12.916873Z",
     "start_time": "2024-12-03T16:42:12.903957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/oscar/scratch/jzhu118/GEDI_Outlier_Detection_OSCAR/Input_files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['GEDI01_B_2022004042652_O17343_04_T10772_02_005_02_V002.h5',\n",
       " 'GEDI01_B_2022207041426_O20491_04_T09293_02_005_03_V002.h5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inDir = os.getcwd() + \"/Input_files\"\n",
    "print(inDir)\n",
    "input_file_names = [g for g in os.listdir(inDir) if g.startswith('GEDI01_B') and g.endswith('.h5')]  # List all GEDI level 2 files in inDir\n",
    "input_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193cf3a589257efc",
   "metadata": {},
   "source": [
    "### Loading files with all information into a huge Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ca866",
   "metadata": {},
   "source": [
    "#### Preprocessing: Get files and sort by beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a37290db29772d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:13.026872Z",
     "start_time": "2024-12-03T16:42:12.947179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: GEDI01_B_2022004042652_O17343_04_T10772_02_005_02_V002.h5\n",
      "The file contains the following groups: ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011', 'METADATA']\n",
      "The file's metadata contains the following attributes: \n",
      "PGEVersion\n",
      "VersionID\n",
      "abstract\n",
      "characterSet\n",
      "creationDate\n",
      "credit\n",
      "fileName\n",
      "language\n",
      "originatorOrganizationName\n",
      "purpose\n",
      "shortName\n",
      "spatialRepresentationType\n",
      "status\n",
      "topicCategory\n",
      "uuid\n",
      "The file contains the following beams: \n",
      "BEAM0000 is a Coverage beam\n",
      "BEAM0001 is a Coverage beam\n",
      "BEAM0010 is a Coverage beam\n",
      "BEAM0011 is a Coverage beam\n",
      "BEAM0101 is a Full power beam\n",
      "BEAM0110 is a Full power beam\n",
      "BEAM1000 is a Full power beam\n",
      "BEAM1011 is a Full power beam\n",
      "Loading file: GEDI01_B_2022207041426_O20491_04_T09293_02_005_03_V002.h5\n",
      "The file contains the following groups: ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011', 'METADATA']\n",
      "The file's metadata contains the following attributes: \n",
      "PGEVersion\n",
      "VersionID\n",
      "abstract\n",
      "characterSet\n",
      "creationDate\n",
      "credit\n",
      "fileName\n",
      "language\n",
      "originatorOrganizationName\n",
      "purpose\n",
      "shortName\n",
      "spatialRepresentationType\n",
      "status\n",
      "topicCategory\n",
      "uuid\n",
      "The file contains the following beams: \n",
      "BEAM0000 is a Coverage beam\n",
      "BEAM0001 is a Coverage beam\n",
      "BEAM0010 is a Coverage beam\n",
      "BEAM0011 is a Coverage beam\n",
      "BEAM0101 is a Full power beam\n",
      "BEAM0110 is a Full power beam\n",
      "BEAM1000 is a Full power beam\n",
      "BEAM1011 is a Full power beam\n"
     ]
    }
   ],
   "source": [
    "input_files = []\n",
    "files_to_beams = dict()\n",
    "for n in input_file_names:\n",
    "    file_path = os.path.join(inDir, n)  # Select an example file\n",
    "    file = h5py.File(file_path, 'r')\n",
    "    input_files.append(file)\n",
    "    \n",
    "    print('Loading file: ' + n)\n",
    "    print('The file contains the following groups: ' + str(list(file.keys())))\n",
    "    \n",
    "    print(\"The file's metadata contains the following attributes: \")\n",
    "    for g in file['METADATA']['DatasetIdentification'].attrs: print(g)\n",
    "    \n",
    "    beamNames = [g for g in file.keys() if g.startswith('BEAM')]\n",
    "    files_to_beams[file] = beamNames\n",
    "    \n",
    "    print(\"The file contains the following beams: \")\n",
    "    for b in beamNames:\n",
    "        print(f\"{b} is a {file[b].attrs['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76c85c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found shot_number dataset with 168553 shots.\n",
      "Features of shape (# shots):\n",
      "  all_samples_sum\n",
      "  altitude_instrument\n",
      "  altitude_instrument_error\n",
      "  beam\n",
      "  bounce_time_offset_bin0\n",
      "  bounce_time_offset_bin0_error\n",
      "  bounce_time_offset_lastbin\n",
      "  bounce_time_offset_lastbin_error\n",
      "  channel\n",
      "  degrade\n",
      "  delta_time\n",
      "  digital_elevation_model\n",
      "  digital_elevation_model_srtm\n",
      "  dynamic_atmosphere_correction\n",
      "  elevation_bin0\n",
      "  elevation_bin0_error\n",
      "  elevation_lastbin\n",
      "  elevation_lastbin_error\n",
      "  geoid\n",
      "  latitude_bin0\n",
      "  latitude_bin0_error\n",
      "  latitude_instrument\n",
      "  latitude_instrument_error\n",
      "  latitude_lastbin\n",
      "  latitude_lastbin_error\n",
      "  local_beam_azimuth\n",
      "  local_beam_azimuth_error\n",
      "  local_beam_elevation\n",
      "  local_beam_elevation_error\n",
      "  longitude_bin0\n",
      "  longitude_bin0_error\n",
      "  longitude_instrument\n",
      "  longitude_instrument_error\n",
      "  longitude_lastbin\n",
      "  longitude_lastbin_error\n",
      "  master_frac\n",
      "  master_int\n",
      "  mean_sea_surface\n",
      "  neutat_delay_derivative_bin0\n",
      "  neutat_delay_derivative_lastbin\n",
      "  neutat_delay_total_bin0\n",
      "  neutat_delay_total_lastbin\n",
      "  noise_mean_corrected\n",
      "  noise_stddev_corrected\n",
      "  nsemean_even\n",
      "  nsemean_odd\n",
      "  range_bias_correction\n",
      "  rx_energy\n",
      "  rx_offset\n",
      "  rx_open\n",
      "  rx_sample_count\n",
      "  rx_sample_start_index\n",
      "  selection_stretchers_x\n",
      "  selection_stretchers_y\n",
      "  shot_number\n",
      "  solar_azimuth\n",
      "  solar_elevation\n",
      "  stale_return_flag\n",
      "  th_left_used\n",
      "  tide_earth\n",
      "  tide_load\n",
      "  tide_ocean\n",
      "  tide_ocean_pole\n",
      "  tide_pole\n",
      "  tx_egamplitude\n",
      "  tx_egamplitude_error\n",
      "  tx_egbias\n",
      "  tx_egbias_error\n",
      "  tx_egflag\n",
      "  tx_eggamma\n",
      "  tx_eggamma_error\n",
      "  tx_egsigma\n",
      "  tx_egsigma_error\n",
      "  tx_gloc\n",
      "  tx_gloc_error\n",
      "  tx_pulseflag\n",
      "  tx_sample_count\n",
      "  tx_sample_start_index\n",
      "Found shot_number dataset with 58496 shots.\n",
      "Features of shape (# shots):\n",
      "  all_samples_sum\n",
      "  altitude_instrument\n",
      "  altitude_instrument_error\n",
      "  beam\n",
      "  bounce_time_offset_bin0\n",
      "  bounce_time_offset_bin0_error\n",
      "  bounce_time_offset_lastbin\n",
      "  bounce_time_offset_lastbin_error\n",
      "  channel\n",
      "  degrade\n",
      "  delta_time\n",
      "  digital_elevation_model\n",
      "  digital_elevation_model_srtm\n",
      "  dynamic_atmosphere_correction\n",
      "  elevation_bin0\n",
      "  elevation_bin0_error\n",
      "  elevation_lastbin\n",
      "  elevation_lastbin_error\n",
      "  geoid\n",
      "  latitude_bin0\n",
      "  latitude_bin0_error\n",
      "  latitude_instrument\n",
      "  latitude_instrument_error\n",
      "  latitude_lastbin\n",
      "  latitude_lastbin_error\n",
      "  local_beam_azimuth\n",
      "  local_beam_azimuth_error\n",
      "  local_beam_elevation\n",
      "  local_beam_elevation_error\n",
      "  longitude_bin0\n",
      "  longitude_bin0_error\n",
      "  longitude_instrument\n",
      "  longitude_instrument_error\n",
      "  longitude_lastbin\n",
      "  longitude_lastbin_error\n",
      "  master_frac\n",
      "  master_int\n",
      "  mean_sea_surface\n",
      "  neutat_delay_derivative_bin0\n",
      "  neutat_delay_derivative_lastbin\n",
      "  neutat_delay_total_bin0\n",
      "  neutat_delay_total_lastbin\n",
      "  noise_mean_corrected\n",
      "  noise_stddev_corrected\n",
      "  nsemean_even\n",
      "  nsemean_odd\n",
      "  range_bias_correction\n",
      "  rx_energy\n",
      "  rx_offset\n",
      "  rx_open\n",
      "  rx_sample_count\n",
      "  rx_sample_start_index\n",
      "  selection_stretchers_x\n",
      "  selection_stretchers_y\n",
      "  shot_number\n",
      "  solar_azimuth\n",
      "  solar_elevation\n",
      "  stale_return_flag\n",
      "  th_left_used\n",
      "  tide_earth\n",
      "  tide_load\n",
      "  tide_ocean\n",
      "  tide_ocean_pole\n",
      "  tide_pole\n",
      "  tx_egamplitude\n",
      "  tx_egamplitude_error\n",
      "  tx_egbias\n",
      "  tx_egbias_error\n",
      "  tx_egflag\n",
      "  tx_eggamma\n",
      "  tx_eggamma_error\n",
      "  tx_egsigma\n",
      "  tx_egsigma_error\n",
      "  tx_gloc\n",
      "  tx_gloc_error\n",
      "  tx_pulseflag\n",
      "  tx_sample_count\n",
      "  tx_sample_start_index\n"
     ]
    }
   ],
   "source": [
    "def list_1d_features(file_path, shot_number_key='shot_number'):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of base feature names from the HDF5 file where each dataset:\n",
    "      - is 1-dimensional and\n",
    "      - has length equal to the number of shots determined from the shot_number dataset.\n",
    "    \n",
    "    Parameters:\n",
    "      file_path (str): Path to the HDF5 file.\n",
    "      shot_number_key (str): Suffix used to locate the shot number dataset.\n",
    "      \n",
    "    Returns:\n",
    "      List[str]: Sorted list of feature base names.\n",
    "    \"\"\"\n",
    "    feature_names = set()\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # First, locate the shot_number dataset to determine the shot count.\n",
    "        shot_num_ds = None\n",
    "        \n",
    "        def find_shot_ds(name, node):\n",
    "            nonlocal shot_num_ds\n",
    "            if isinstance(node, h5py.Dataset) and name.endswith(shot_number_key):\n",
    "                shot_num_ds = node\n",
    "        \n",
    "        f.visititems(find_shot_ds)\n",
    "        \n",
    "        if shot_num_ds is None:\n",
    "            print(f\"Error: Could not find a dataset ending with '{shot_number_key}'.\")\n",
    "            return []\n",
    "        \n",
    "        shot_count = shot_num_ds.shape[0]\n",
    "        print(f\"Found shot_number dataset with {shot_count} shots.\")\n",
    "        \n",
    "        # Now, visit every dataset and check if it is 1D with length equal to shot_count.\n",
    "        def visitor_func(name, node):\n",
    "            if isinstance(node, h5py.Dataset):\n",
    "                shape = node.shape\n",
    "                if len(shape) == 1 and shape[0] == shot_count:\n",
    "                    # Extract the base name (the part after the last '/')\n",
    "                    base_name = name.split('/')[-1]\n",
    "                    feature_names.add(base_name)\n",
    "        \n",
    "        f.visititems(visitor_func)\n",
    "    \n",
    "    return sorted(feature_names)\n",
    "\n",
    "# Example usage:\n",
    "for f in input_file_names:\n",
    "    file_path = file_path = os.path.join(inDir, f)\n",
    "    features = list_1d_features(file_path)\n",
    "    print(\"Features of shape (# shots):\")\n",
    "    for feat in features:\n",
    "        print(f\"  {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_features_by_shot_count(file_path, shot_number_key='shot_number'):\n",
    "    \"\"\"\n",
    "    Returns a list of dataset names from the given HDF5 file that have shapes matching:\n",
    "      - (# shots,) or\n",
    "      - (# shots, n) or (n, # shots)\n",
    "    \n",
    "    Parameters:\n",
    "      file_path (str): Path to the HDF5 file.\n",
    "      shot_number_key (str): Key name (or suffix) used to identify the shot number dataset.\n",
    "    \n",
    "    Returns:\n",
    "      List[str]: List of dataset names that meet the criteria.\n",
    "    \"\"\"\n",
    "    matching_features = []\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # First, locate the shot_number dataset to determine the number of shots.\n",
    "        shot_num_ds = None\n",
    "        def find_shot_ds(name, node):\n",
    "            nonlocal shot_num_ds\n",
    "            if isinstance(node, h5py.Dataset) and name.endswith(shot_number_key):\n",
    "                shot_num_ds = node\n",
    "        f.visititems(find_shot_ds)\n",
    "        \n",
    "        if shot_num_ds is None:\n",
    "            print(f\"Error: Could not find a dataset ending with '{shot_number_key}'.\")\n",
    "            return matching_features\n",
    "        \n",
    "        shot_count = shot_num_ds.shape[0]\n",
    "        print(f\"Found shot_number dataset with {shot_count} shots.\")\n",
    "\n",
    "        # Now, traverse all datasets and check their shapes.\n",
    "        def visitor_func(name, node):\n",
    "            if isinstance(node, h5py.Dataset):\n",
    "                shape = node.shape\n",
    "                # Check for 1D dataset with length equal to shot_count.\n",
    "                if len(shape) == 1 and shape[0] == shot_count:\n",
    "                    matching_features.append(name)\n",
    "                # Check for 2D dataset where one of the dimensions equals shot_count.\n",
    "                elif len(shape) == 2 and (shape[0] == shot_count or shape[1] == shot_count):\n",
    "                    matching_features.append(name)\n",
    "        \n",
    "        f.visititems(visitor_func)\n",
    "    \n",
    "    return matching_features\n",
    "\n",
    "# Example usage:\n",
    "for f in input_file_names:\n",
    "    file_path = file_path = os.path.join(inDir, f)\n",
    "    features = list_features_by_shot_count(file_path)\n",
    "    print(\"Features matching the shot count criteria:\")\n",
    "    for feature in features:\n",
    "        print(f\"  {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14096b",
   "metadata": {},
   "source": [
    "#### Helper functions for the main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660146583f485f77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:14.006557Z",
     "start_time": "2024-12-03T16:42:13.992165Z"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def collect_all_datasets(file_path):\n",
    "    \"\"\"\n",
    "    Recursively collects all dataset paths within the HDF5 file.\n",
    "    Returns a list of dataset path strings.\n",
    "    \"\"\"\n",
    "    dataset_paths = []\n",
    "    \n",
    "    def visitor_func(name, node):\n",
    "        if isinstance(node, h5py.Dataset):\n",
    "            dataset_paths.append(name)\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        f.visititems(visitor_func)\n",
    "    \n",
    "    return dataset_paths\n",
    "\n",
    "def get_dataset_by_name(h5_file, beam_ds, name):\n",
    "    \"\"\"\n",
    "    Given an open HDF5 file (h5_file) and a list of dataset paths (beam_ds),\n",
    "    returns the data for the first dataset whose path ends with '/{name}'.\n",
    "    If not found, returns None.\n",
    "    \"\"\"\n",
    "    candidates = [ds for ds in beam_ds if ds.endswith(f'/{name}')]\n",
    "    if not candidates:\n",
    "        print(f\"Warning: No dataset ending with '/{name}' found.\")\n",
    "        return None\n",
    "    dataset_path = candidates[0]\n",
    "    return h5_file[dataset_path][()]  # Read dataset into memory\n",
    "\n",
    "def load_attributes_to_dict(txt_file_path):\n",
    "    \"\"\"\n",
    "    Reads attribute names from a text file (one per line) and returns a dictionary.\n",
    "    This dictionary maps the human-friendly column name to the dataset suffix.\n",
    "    \"\"\"\n",
    "    with open(txt_file_path, 'r') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    attr_dict = {}\n",
    "    for raw_attr in lines:\n",
    "        attr_dict[raw_attr] = raw_attr  # Direct mapping for simplicity\n",
    "    return attr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61cfd0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded L1B attributes: {'all_samples_sum': 'all_samples_sum', 'beam': 'beam', 'channel': 'channel', 'master_frac': 'master_frac', 'master_int': 'master_int', 'noise_mean_corrected': 'noise_mean_corrected', 'noise_stddev_corrected': 'noise_stddev_corrected', 'nsemean_even': 'nsemean_even', 'nsemean_odd': 'nsemean_odd', 'rx_energy': 'rx_energy', 'rx_offset': 'rx_offset', 'rx_open': 'rx_open', 'rx_sample_count': 'rx_sample_count', 'rx_sample_start_index': 'rx_sample_start_index', 'selection_stretchers_x': 'selection_stretchers_x', 'selection_stretchers_y': 'selection_stretchers_y', 'shot_number': 'shot_number', 'stale_return_flag': 'stale_return_flag', 'th_left_used': 'th_left_used', 'tx_egamplitude': 'tx_egamplitude', 'tx_egamplitude_error': 'tx_egamplitude_error', 'tx_egbias': 'tx_egbias', 'tx_egbias_error': 'tx_egbias_error', 'tx_egflag': 'tx_egflag', 'tx_eggamma': 'tx_eggamma', 'tx_eggamma_error': 'tx_eggamma_error', 'tx_egsigma': 'tx_egsigma', 'tx_egsigma_error': 'tx_egsigma_error', 'tx_gloc': 'tx_gloc', 'tx_gloc_error': 'tx_gloc_error', 'tx_pulseflag': 'tx_pulseflag', 'tx_sample_count': 'tx_sample_count', 'tx_sample_start_index': 'tx_sample_start_index', 'altitude_instrument': 'altitude_instrument', 'altitude_instrument_error': 'altitude_instrument_error', 'bounce_time_offset_bin0': 'bounce_time_offset_bin0', 'bounce_time_offset_bin0_error': 'bounce_time_offset_bin0_error', 'bounce_time_offset_lastbin': 'bounce_time_offset_lastbin', 'bounce_time_offset_lastbin_error': 'bounce_time_offset_lastbin_error', 'degrade': 'degrade', 'delta_time': 'delta_time', 'digital_elevation_model': 'digital_elevation_model', 'elevation_bin0': 'elevation_bin0', 'elevation_bin0_error': 'elevation_bin0_error', 'elevation_lastbin': 'elevation_lastbin', 'elevation_lastbin_error': 'elevation_lastbin_error', 'latitude_bin0': 'latitude_bin0', 'latitude_bin0_error': 'latitude_bin0_error', 'latitude_lastbin': 'latitude_lastbin', 'latitude_lastbin_error': 'latitude_lastbin_error', 'latitude_instrument': 'latitude_instrument', 'latitude_instrument_error': 'latitude_instrument_error', 'local_beam_azimuth': 'local_beam_azimuth', 'local_beam_azimuth_error': 'local_beam_azimuth_error', 'local_beam_elevation': 'local_beam_elevation', 'local_beam_elevation_error': 'local_beam_elevation_error', 'longitude_bin0': 'longitude_bin0', 'longitude_bin0_error': 'longitude_bin0_error', 'longitude_lastbin': 'longitude_lastbin', 'longitude_lastbin_error': 'longitude_lastbin_error', 'longitude_instrument': 'longitude_instrument', 'longitude_instrument_error': 'longitude_instrument_error', 'mean_sea_surface': 'mean_sea_surface', 'neutat_delay_derivative_bin0': 'neutat_delay_derivative_bin0', 'neutat_delay_derivative_lastbin': 'neutat_delay_derivative_lastbin', 'neutat_delay_total_bin0': 'neutat_delay_total_bin0', 'neutat_delay_total_lastbin': 'neutat_delay_total_lastbin', 'range_bias_correction': 'range_bias_correction', 'solar_azimuth': 'solar_azimuth', 'solar_elevation': 'solar_elevation', 'dynamic_atmosphere_correction': 'dynamic_atmosphere_correction', 'geoid': 'geoid', 'tide_earth': 'tide_earth', 'tide_load': 'tide_load', 'tide_ocean': 'tide_ocean', 'tide_ocean_pole': 'tide_ocean_pole', 'tide_pole': 'tide_pole'}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# LOAD L1B FEATURES\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Use the new features list for L1B\n",
    "txt_path = 'L1B_features.txt'\n",
    "ATTRIBUTES_TO_LOAD = load_attributes_to_dict(txt_path)\n",
    "print(\"Loaded L1B attributes:\", ATTRIBUTES_TO_LOAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4574d5",
   "metadata": {},
   "source": [
    "#### Main loop, collects all target features and puts them inside a Pandas dataframe.\n",
    "WARNING: Despite my best attempts at optimization, this is *extremely* memory intensive even for one 5GB HDF5 file. Only run on a supercomputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbe575445c09a1d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:50:27.138768Z",
     "start_time": "2024-12-03T17:37:18.265405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing L1B file: /oscar/scratch/jzhu118/GEDI_Outlier_Detection_OSCAR/Input_files/GEDI01_B_2022004042652_O17343_04_T10772_02_005_02_V002.h5\n",
      "  Processing beam: BEAM0000\n",
      "    Number of shots: 168553\n",
      "surface_type shape: (5, 168553)\n",
      "Transposed surface_type shape: (168553, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM0000 with 168553 shots.\n",
      "  Processing beam: BEAM0001\n",
      "    Number of shots: 168565\n",
      "surface_type shape: (5, 168565)\n",
      "Transposed surface_type shape: (168565, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM0001 with 168565 shots.\n",
      "  Processing beam: BEAM0010\n",
      "    Number of shots: 168671\n",
      "surface_type shape: (5, 168671)\n",
      "Transposed surface_type shape: (168671, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM0010 with 168671 shots.\n",
      "  Processing beam: BEAM0011\n",
      "    Number of shots: 168649\n",
      "surface_type shape: (5, 168649)\n",
      "Transposed surface_type shape: (168649, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM0011 with 168649 shots.\n",
      "  Processing beam: BEAM0101\n",
      "    Number of shots: 168581\n",
      "surface_type shape: (5, 168581)\n",
      "Transposed surface_type shape: (168581, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM0101 with 168581 shots.\n",
      "  Processing beam: BEAM0110\n",
      "    Number of shots: 168373\n",
      "surface_type shape: (5, 168373)\n",
      "Transposed surface_type shape: (168373, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM0110 with 168373 shots.\n",
      "  Processing beam: BEAM1000\n",
      "    Number of shots: 168351\n",
      "surface_type shape: (5, 168351)\n",
      "Transposed surface_type shape: (168351, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM1000 with 168351 shots.\n",
      "  Processing beam: BEAM1011\n",
      "    Number of shots: 168553\n",
      "surface_type shape: (5, 168553)\n",
      "Transposed surface_type shape: (168553, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM1011 with 168553 shots.\n",
      "\n",
      "Processing L1B file: /oscar/scratch/jzhu118/GEDI_Outlier_Detection_OSCAR/Input_files/GEDI01_B_2022207041426_O20491_04_T09293_02_005_03_V002.h5\n",
      "  Processing beam: BEAM0000\n",
      "    Number of shots: 59656\n",
      "surface_type shape: (5, 59656)\n",
      "Transposed surface_type shape: (59656, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM0000 with 59656 shots.\n",
      "  Processing beam: BEAM0001\n",
      "    Number of shots: 58340\n",
      "surface_type shape: (5, 58340)\n",
      "Transposed surface_type shape: (58340, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM0001 with 58340 shots.\n",
      "  Processing beam: BEAM0010\n",
      "    Number of shots: 59659\n",
      "surface_type shape: (5, 59659)\n",
      "Transposed surface_type shape: (59659, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM0010 with 59659 shots.\n",
      "  Processing beam: BEAM0011\n",
      "    Number of shots: 58368\n",
      "surface_type shape: (5, 58368)\n",
      "Transposed surface_type shape: (58368, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM0011 with 58368 shots.\n",
      "  Processing beam: BEAM0101\n",
      "    Number of shots: 58409\n",
      "surface_type shape: (5, 58409)\n",
      "Transposed surface_type shape: (58409, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM0101 with 58409 shots.\n",
      "  Processing beam: BEAM0110\n",
      "    Number of shots: 59679\n",
      "surface_type shape: (5, 59679)\n",
      "Transposed surface_type shape: (59679, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM0110 with 59679 shots.\n",
      "  Processing beam: BEAM1000\n",
      "    Number of shots: 59747\n",
      "surface_type shape: (5, 59747)\n",
      "Transposed surface_type shape: (59747, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM1000 with 59747 shots.\n",
      "  Processing beam: BEAM1011\n",
      "    Number of shots: 58496\n",
      "surface_type shape: (5, 58496)\n",
      "Transposed surface_type shape: (58496, 5)\n",
      "Successfully processed surface_type!\n",
      "    Processed beam BEAM1011 with 58496 shots.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# PROCESS L1B FILES\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Process each L1B HDF5 file (assumed pre-loaded into input_files)\n",
    "for f in input_files:\n",
    "    print(f\"\\nProcessing L1B file: {f.filename}\")\n",
    "    \n",
    "    # Collect all dataset paths in the current file\n",
    "    all_ds_for_file = collect_all_datasets(f.filename)\n",
    "    \n",
    "    # Process each beam in the file\n",
    "    for b in files_to_beams[f]:\n",
    "        print(f\"  Processing beam: {b}\")\n",
    "        \n",
    "        # Filter dataset paths to only those belonging to the current beam\n",
    "        beam_ds = [ds for ds in all_ds_for_file if b in ds]\n",
    "        \n",
    "        # 1) Retrieve the shot_number dataset (1D array; one entry per shot)\n",
    "        shotNums = get_dataset_by_name(f, beam_ds, 'shot_number')\n",
    "        if shotNums is None:\n",
    "            print(f\"    Warning: No 'shot_number' dataset found for beam {b}. Skipping beam.\")\n",
    "            continue\n",
    "        shotNums = np.array(shotNums)\n",
    "        row_count = len(shotNums)\n",
    "        print(f\"    Number of shots: {row_count}\")\n",
    "        \n",
    "        # 2) Build constant columns: File Name and Beam Name\n",
    "        file_name_col = [os.path.basename(f.filename)] * row_count\n",
    "        beam_name_col = [b] * row_count\n",
    "        \n",
    "        # 3) Extract all 1D features specified in the L1B features list\n",
    "        attr_data = {}\n",
    "        for col_label, ds_suffix in ATTRIBUTES_TO_LOAD.items():\n",
    "            data_array = get_dataset_by_name(f, beam_ds, ds_suffix)\n",
    "            if data_array is None:\n",
    "                print(f\"    Warning: Missing dataset for {ds_suffix}. Filling with None.\")\n",
    "                attr_data[col_label] = np.full(row_count, None)\n",
    "            else:\n",
    "                data_array = np.array(data_array)\n",
    "                if len(data_array) != row_count:\n",
    "                    print(f\"    Warning: Row count mismatch for {col_label} in beam {b}.\")\n",
    "                attr_data[col_label] = data_array\n",
    "        \n",
    "        # 4) Extract the 'surface_type' feature (2D array with shape (5, # of shots))\n",
    "        surface_data = get_dataset_by_name(f, beam_ds, 'surface_type')\n",
    "        if surface_data is None:\n",
    "            print(\"    Warning: 'surface_type' dataset missing. Filling with NaNs.\")\n",
    "            # Create five columns filled with NaN\n",
    "            surface_cols = {\n",
    "                'surface_land': np.full(row_count, np.nan),\n",
    "                'surface_ocean': np.full(row_count, np.nan),\n",
    "                'surface_sea_ice': np.full(row_count, np.nan),\n",
    "                'surface_land_ice': np.full(row_count, np.nan),\n",
    "                'surface_inland_water': np.full(row_count, np.nan)\n",
    "            }\n",
    "        else:\n",
    "            surface_data = np.array(surface_data)\n",
    "            print(f'surface_type shape: {np.shape(surface_data)}')\n",
    "            # Check the shape and transpose if needed so that shape becomes (row_count, 5)\n",
    "            if surface_data.shape[0] == 5 and surface_data.shape[1] == row_count:\n",
    "                surface_data = surface_data.T\n",
    "            elif surface_data.shape[0] != row_count or surface_data.shape[1] != 5:\n",
    "                print(\"    Warning: Unexpected shape for 'surface_type'. Expected (5, #shots) or (#shots, 5).\")\n",
    "            print(f'Transposed surface_type shape: {np.shape(surface_data)}')\n",
    "            # Create interpretable column labels for surface types\n",
    "            surface_cols = {\n",
    "                'surface_land': surface_data[:, 0],\n",
    "                'surface_ocean': surface_data[:, 1],\n",
    "                'surface_sea_ice': surface_data[:, 2],\n",
    "                'surface_land_ice': surface_data[:, 3],\n",
    "                'surface_inland_water': surface_data[:, 4]\n",
    "            }\n",
    "            print('Successfully processed surface_type!')\n",
    "        \n",
    "        # 5) Construct the DataFrame for the current beam by combining all columns\n",
    "        df = pd.DataFrame({\n",
    "            'File Name': file_name_col,\n",
    "            'Beam Name': beam_name_col,\n",
    "            'Shot Number': shotNums,\n",
    "            **attr_data,\n",
    "            **surface_cols\n",
    "        })\n",
    "        \n",
    "        # 6) One-hot encode categorical features, e.g., Beam Name (and Channel if present)\n",
    "        if 'Beam Name' in df.columns:\n",
    "            df = pd.get_dummies(df, columns=['Beam Name'], prefix='')\n",
    "        if 'channel' in df.columns:\n",
    "            df = pd.get_dummies(df, columns=['channel'], prefix='channel')\n",
    "        \n",
    "        dataframes.append(df)\n",
    "        print(f\"    Processed beam {b} with {row_count} shots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddae7008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame shape: (1820650, 97)\n",
      "Successfully wrote to a .parquet file!\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# FINALIZE: Concatenate all partial DataFrames into one complete DataFrame\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if dataframes:\n",
    "    complete_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\nFinal DataFrame shape: {complete_df.shape}\")\n",
    "else:\n",
    "    complete_df = pd.DataFrame()\n",
    "    print(\"\\nNo data extracted from L1B files.\")\n",
    "\n",
    "# Optionally, save the complete DataFrame for further processing:\n",
    "complete_df.to_parquet(\"L1B_raw.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "print('Successfully wrote to a .parquet file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73cf29",
   "metadata": {},
   "source": [
    "#### Feature engineering\n",
    "1. Adding Z-Scores for five most relevant RH metrics\n",
    "2. Creating RH50 / RH100 ratio\n",
    "3. Adding RH95 - RH50\n",
    "4. Adding \"Missingness\" - the number of NaNs in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zeros = (complete_df['RH_100'] == 0).sum()\n",
    "print(f\"Number of zeros in RH_100: {num_zeros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ed4109accefc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T15:26:48.589282Z",
     "start_time": "2024-10-28T15:26:48.291657Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Add Z-Scores of the five RH metrics'''\n",
    "rh_nums = [25, 50, 75, 85, 95, 100]\n",
    "for i in rh_nums:\n",
    "    col_name = f'RH_{i}'\n",
    "    complete_df[f'{col_name} Z Score'] = (complete_df[col_name] - complete_df[col_name].mean()) / complete_df[col_name].std()\n",
    "\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a756676",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding the RH_50 / RH_100 feature'''\n",
    "complete_df['RH_50_v_100'] = complete_df['RH_50'] / complete_df['RH_100']\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding the (RH95 - RH50) feature'''\n",
    "rh_50 = complete_df['RH_50']\n",
    "rh_95 = complete_df['RH_95']\n",
    "complete_df['RH_95_minus_50'] = (rh_95 - rh_50)\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding the 'Missingness' feature'''\n",
    "# Count the number of NaNs in each row\n",
    "complete_df['Missingness'] = complete_df.isna().sum(axis=1)\n",
    "\n",
    "\n",
    "# Optionally, inspect how many rows have missing data\n",
    "print('Number of NaNs:')\n",
    "print(complete_df['Missingness'].value_counts())\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd1187d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate columns: Index([], dtype='object')\n",
      "There are 0 duplicated columns.\n",
      "NaN value count:\n",
      "\n",
      "File Name              0\n",
      "Shot Number            0\n",
      "RH_25                  0\n",
      "RH_50                  0\n",
      "RH_75                  0\n",
      "                  ...   \n",
      "channel_3        1885151\n",
      "beam_BEAM1000    1893679\n",
      "channel_4        1893679\n",
      "beam_BEAM1011    1894845\n",
      "channel_5        1894845\n",
      "Length: 100, dtype: int64\n",
      "Successfully wrote to a .parquet file!\n"
     ]
    }
   ],
   "source": [
    "'''Optional: Save to a parquet file'''\n",
    "# Get a list of column names\n",
    "columns = complete_df.columns\n",
    "\n",
    "# Find duplicates\n",
    "duplicate_columns = columns[columns.duplicated()]\n",
    "print(\"Duplicate columns:\", duplicate_columns)\n",
    "print('There are ' + str(columns.duplicated().sum()) + ' duplicated columns.')\n",
    "print('NaN value count:\\n')\n",
    "print(complete_df.isnull().sum())\n",
    "# complete_df.describe()\n",
    "\n",
    "# Uncomment the two lines below to write to parquet\n",
    "complete_df.to_parquet(\"TEST.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "print('Successfully wrote to a .parquet file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307ec82",
   "metadata": {},
   "source": [
    "#### This is the filtering step for PCA if there are any NaN rows in the dataframe\n",
    "Note: there should not be mnany NaN values if you accounted for the different shapes of the data inside the HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b51cc6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Beam Name</th>\n",
       "      <th>Shot Number</th>\n",
       "      <th>RH_25</th>\n",
       "      <th>RH_50</th>\n",
       "      <th>RH_75</th>\n",
       "      <th>RH_85</th>\n",
       "      <th>RH_95</th>\n",
       "      <th>RH_100</th>\n",
       "      <th>channel</th>\n",
       "      <th>...</th>\n",
       "      <th>zcross_localenergy</th>\n",
       "      <th>RH_25 Z Score</th>\n",
       "      <th>RH_50 Z Score</th>\n",
       "      <th>RH_75 Z Score</th>\n",
       "      <th>RH_85 Z Score</th>\n",
       "      <th>RH_95 Z Score</th>\n",
       "      <th>RH_100 Z Score</th>\n",
       "      <th>RH_50_v_100</th>\n",
       "      <th>RH_95_minus_50</th>\n",
       "      <th>Missingness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/oscar/scratch/jzhu118/GEDI_Outlier_Detection_...</td>\n",
       "      <td>BEAM0000</td>\n",
       "      <td>20820000200050304</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>163.479019</td>\n",
       "      <td>-0.267243</td>\n",
       "      <td>-0.224966</td>\n",
       "      <td>-0.212232</td>\n",
       "      <td>-0.213468</td>\n",
       "      <td>-0.214492</td>\n",
       "      <td>-0.240572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/oscar/scratch/jzhu118/GEDI_Outlier_Detection_...</td>\n",
       "      <td>BEAM0000</td>\n",
       "      <td>20820000200050305</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>178.583328</td>\n",
       "      <td>-0.352941</td>\n",
       "      <td>-0.243600</td>\n",
       "      <td>-0.212232</td>\n",
       "      <td>-0.206578</td>\n",
       "      <td>-0.202634</td>\n",
       "      <td>-0.216597</td>\n",
       "      <td>-0.027559</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/oscar/scratch/jzhu118/GEDI_Outlier_Detection_...</td>\n",
       "      <td>BEAM0000</td>\n",
       "      <td>20820000200050306</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.72</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>203.563889</td>\n",
       "      <td>-0.303326</td>\n",
       "      <td>-0.243600</td>\n",
       "      <td>-0.225654</td>\n",
       "      <td>-0.225526</td>\n",
       "      <td>-0.224867</td>\n",
       "      <td>-0.245900</td>\n",
       "      <td>-0.030172</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/oscar/scratch/jzhu118/GEDI_Outlier_Detection_...</td>\n",
       "      <td>BEAM0000</td>\n",
       "      <td>20820000200050307</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>221.641510</td>\n",
       "      <td>-0.303326</td>\n",
       "      <td>-0.243600</td>\n",
       "      <td>-0.219902</td>\n",
       "      <td>-0.218636</td>\n",
       "      <td>-0.214492</td>\n",
       "      <td>-0.225921</td>\n",
       "      <td>-0.028340</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/oscar/scratch/jzhu118/GEDI_Outlier_Detection_...</td>\n",
       "      <td>BEAM0000</td>\n",
       "      <td>20820000200050308</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.94</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>268.898804</td>\n",
       "      <td>-0.316857</td>\n",
       "      <td>-0.232952</td>\n",
       "      <td>-0.212232</td>\n",
       "      <td>-0.206578</td>\n",
       "      <td>-0.192258</td>\n",
       "      <td>-0.191289</td>\n",
       "      <td>-0.010989</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11607182</th>\n",
       "      <td>/oscar/scratch/jzhu118/GEDI_Outlier_Detection_...</td>\n",
       "      <td>BEAM1011</td>\n",
       "      <td>224961100200293934</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.02</td>\n",
       "      <td>3.70</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2616.521729</td>\n",
       "      <td>-0.470212</td>\n",
       "      <td>-0.272881</td>\n",
       "      <td>-0.219902</td>\n",
       "      <td>-0.206578</td>\n",
       "      <td>-0.180400</td>\n",
       "      <td>-0.062087</td>\n",
       "      <td>-0.048649</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11607183</th>\n",
       "      <td>/oscar/scratch/jzhu118/GEDI_Outlier_Detection_...</td>\n",
       "      <td>BEAM1011</td>\n",
       "      <td>224961100200293935</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.70</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2879.136963</td>\n",
       "      <td>-0.470212</td>\n",
       "      <td>-0.283529</td>\n",
       "      <td>-0.225654</td>\n",
       "      <td>-0.218636</td>\n",
       "      <td>-0.192258</td>\n",
       "      <td>-0.062087</td>\n",
       "      <td>-0.059459</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11607184</th>\n",
       "      <td>/oscar/scratch/jzhu118/GEDI_Outlier_Detection_...</td>\n",
       "      <td>BEAM1011</td>\n",
       "      <td>224961100200293936</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.02</td>\n",
       "      <td>3.85</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2619.544189</td>\n",
       "      <td>-0.488253</td>\n",
       "      <td>-0.283529</td>\n",
       "      <td>-0.219902</td>\n",
       "      <td>-0.206578</td>\n",
       "      <td>-0.180400</td>\n",
       "      <td>-0.042108</td>\n",
       "      <td>-0.057143</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11607185</th>\n",
       "      <td>/oscar/scratch/jzhu118/GEDI_Outlier_Detection_...</td>\n",
       "      <td>BEAM1011</td>\n",
       "      <td>224961100200293937</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.81</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2381.172363</td>\n",
       "      <td>-0.438639</td>\n",
       "      <td>-0.262233</td>\n",
       "      <td>-0.212232</td>\n",
       "      <td>-0.199688</td>\n",
       "      <td>-0.175953</td>\n",
       "      <td>-0.047435</td>\n",
       "      <td>-0.036745</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11607186</th>\n",
       "      <td>/oscar/scratch/jzhu118/GEDI_Outlier_Detection_...</td>\n",
       "      <td>BEAM1011</td>\n",
       "      <td>224961100200293938</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.74</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2977.040283</td>\n",
       "      <td>-0.470212</td>\n",
       "      <td>-0.283529</td>\n",
       "      <td>-0.225654</td>\n",
       "      <td>-0.213468</td>\n",
       "      <td>-0.192258</td>\n",
       "      <td>-0.056759</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11607187 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  File Name Beam Name  \\\n",
       "0         /oscar/scratch/jzhu118/GEDI_Outlier_Detection_...  BEAM0000   \n",
       "1         /oscar/scratch/jzhu118/GEDI_Outlier_Detection_...  BEAM0000   \n",
       "2         /oscar/scratch/jzhu118/GEDI_Outlier_Detection_...  BEAM0000   \n",
       "3         /oscar/scratch/jzhu118/GEDI_Outlier_Detection_...  BEAM0000   \n",
       "4         /oscar/scratch/jzhu118/GEDI_Outlier_Detection_...  BEAM0000   \n",
       "...                                                     ...       ...   \n",
       "11607182  /oscar/scratch/jzhu118/GEDI_Outlier_Detection_...  BEAM1011   \n",
       "11607183  /oscar/scratch/jzhu118/GEDI_Outlier_Detection_...  BEAM1011   \n",
       "11607184  /oscar/scratch/jzhu118/GEDI_Outlier_Detection_...  BEAM1011   \n",
       "11607185  /oscar/scratch/jzhu118/GEDI_Outlier_Detection_...  BEAM1011   \n",
       "11607186  /oscar/scratch/jzhu118/GEDI_Outlier_Detection_...  BEAM1011   \n",
       "\n",
       "                 Shot Number  RH_25  RH_50  RH_75  RH_85  RH_95  RH_100  \\\n",
       "0          20820000200050304  -0.78   0.00   0.78   1.19   1.79    2.36   \n",
       "1          20820000200050305  -0.97  -0.07   0.78   1.23   1.87    2.54   \n",
       "2          20820000200050306  -0.86  -0.07   0.71   1.12   1.72    2.32   \n",
       "3          20820000200050307  -0.86  -0.07   0.74   1.16   1.79    2.47   \n",
       "4          20820000200050308  -0.89  -0.03   0.78   1.23   1.94    2.73   \n",
       "...                      ...    ...    ...    ...    ...    ...     ...   \n",
       "11607182  224961100200293934  -1.23  -0.18   0.74   1.23   2.02    3.70   \n",
       "11607183  224961100200293935  -1.23  -0.22   0.71   1.16   1.94    3.70   \n",
       "11607184  224961100200293936  -1.27  -0.22   0.74   1.23   2.02    3.85   \n",
       "11607185  224961100200293937  -1.16  -0.14   0.78   1.27   2.05    3.81   \n",
       "11607186  224961100200293938  -1.23  -0.22   0.71   1.19   1.94    3.74   \n",
       "\n",
       "          channel  ...  zcross_localenergy  RH_25 Z Score  RH_50 Z Score  \\\n",
       "0               0  ...          163.479019      -0.267243      -0.224966   \n",
       "1               0  ...          178.583328      -0.352941      -0.243600   \n",
       "2               0  ...          203.563889      -0.303326      -0.243600   \n",
       "3               0  ...          221.641510      -0.303326      -0.243600   \n",
       "4               0  ...          268.898804      -0.316857      -0.232952   \n",
       "...           ...  ...                 ...            ...            ...   \n",
       "11607182        5  ...         2616.521729      -0.470212      -0.272881   \n",
       "11607183        5  ...         2879.136963      -0.470212      -0.283529   \n",
       "11607184        5  ...         2619.544189      -0.488253      -0.283529   \n",
       "11607185        5  ...         2381.172363      -0.438639      -0.262233   \n",
       "11607186        5  ...         2977.040283      -0.470212      -0.283529   \n",
       "\n",
       "          RH_75 Z Score  RH_85 Z Score  RH_95 Z Score  RH_100 Z Score  \\\n",
       "0             -0.212232      -0.213468      -0.214492       -0.240572   \n",
       "1             -0.212232      -0.206578      -0.202634       -0.216597   \n",
       "2             -0.225654      -0.225526      -0.224867       -0.245900   \n",
       "3             -0.219902      -0.218636      -0.214492       -0.225921   \n",
       "4             -0.212232      -0.206578      -0.192258       -0.191289   \n",
       "...                 ...            ...            ...             ...   \n",
       "11607182      -0.219902      -0.206578      -0.180400       -0.062087   \n",
       "11607183      -0.225654      -0.218636      -0.192258       -0.062087   \n",
       "11607184      -0.219902      -0.206578      -0.180400       -0.042108   \n",
       "11607185      -0.212232      -0.199688      -0.175953       -0.047435   \n",
       "11607186      -0.225654      -0.213468      -0.192258       -0.056759   \n",
       "\n",
       "          RH_50_v_100  RH_95_minus_50  Missingness  \n",
       "0            0.000000            1.79            0  \n",
       "1           -0.027559            1.94            0  \n",
       "2           -0.030172            1.79            0  \n",
       "3           -0.028340            1.86            0  \n",
       "4           -0.010989            1.97            0  \n",
       "...               ...             ...          ...  \n",
       "11607182    -0.048649            2.20            0  \n",
       "11607183    -0.059459            2.16            0  \n",
       "11607184    -0.057143            2.24            0  \n",
       "11607185    -0.036745            2.19            0  \n",
       "11607186    -0.058824            2.16            0  \n",
       "\n",
       "[11607187 rows x 97 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Optional: load complete_df from .parquet file'''\n",
    "complete_df = pd.read_parquet('input_raw.parquet', engine='pyarrow')\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e344202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe shape: (11607187, 97)\n",
      "Shot numbers dropped\n",
      "Dataframe shape after dropping NaN columns: (11607187, 87)\n",
      "Non-numeric columns removed from dataframe\n",
      "Cleaned dataframe size: (11607187, 85)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RH_25</th>\n",
       "      <th>RH_50</th>\n",
       "      <th>RH_75</th>\n",
       "      <th>RH_85</th>\n",
       "      <th>RH_95</th>\n",
       "      <th>RH_100</th>\n",
       "      <th>channel</th>\n",
       "      <th>degrade_flag</th>\n",
       "      <th>delta_time</th>\n",
       "      <th>digital_elevation_model</th>\n",
       "      <th>...</th>\n",
       "      <th>zcross_amp</th>\n",
       "      <th>zcross_localenergy</th>\n",
       "      <th>RH_25 Z Score</th>\n",
       "      <th>RH_50 Z Score</th>\n",
       "      <th>RH_75 Z Score</th>\n",
       "      <th>RH_85 Z Score</th>\n",
       "      <th>RH_95 Z Score</th>\n",
       "      <th>RH_100 Z Score</th>\n",
       "      <th>RH_95_minus_50</th>\n",
       "      <th>Missingness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.146742e+07</td>\n",
       "      <td>-999999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>296.142761</td>\n",
       "      <td>163.479019</td>\n",
       "      <td>-0.267243</td>\n",
       "      <td>-0.224966</td>\n",
       "      <td>-0.212232</td>\n",
       "      <td>-0.213468</td>\n",
       "      <td>-0.214492</td>\n",
       "      <td>-0.240572</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.146742e+07</td>\n",
       "      <td>-999999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>302.973358</td>\n",
       "      <td>178.583328</td>\n",
       "      <td>-0.352941</td>\n",
       "      <td>-0.243600</td>\n",
       "      <td>-0.212232</td>\n",
       "      <td>-0.206578</td>\n",
       "      <td>-0.202634</td>\n",
       "      <td>-0.216597</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.86</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.72</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.146742e+07</td>\n",
       "      <td>-999999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>302.514313</td>\n",
       "      <td>203.563889</td>\n",
       "      <td>-0.303326</td>\n",
       "      <td>-0.243600</td>\n",
       "      <td>-0.225654</td>\n",
       "      <td>-0.225526</td>\n",
       "      <td>-0.224867</td>\n",
       "      <td>-0.245900</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.86</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.146742e+07</td>\n",
       "      <td>-999999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>309.622925</td>\n",
       "      <td>221.641510</td>\n",
       "      <td>-0.303326</td>\n",
       "      <td>-0.243600</td>\n",
       "      <td>-0.219902</td>\n",
       "      <td>-0.218636</td>\n",
       "      <td>-0.214492</td>\n",
       "      <td>-0.225921</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.89</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.94</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.146742e+07</td>\n",
       "      <td>-999999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>320.783600</td>\n",
       "      <td>268.898804</td>\n",
       "      <td>-0.316857</td>\n",
       "      <td>-0.232952</td>\n",
       "      <td>-0.212232</td>\n",
       "      <td>-0.206578</td>\n",
       "      <td>-0.192258</td>\n",
       "      <td>-0.191289</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11607182</th>\n",
       "      <td>-1.23</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.02</td>\n",
       "      <td>3.70</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.552156e+08</td>\n",
       "      <td>254.929123</td>\n",
       "      <td>...</td>\n",
       "      <td>1057.588623</td>\n",
       "      <td>2616.521729</td>\n",
       "      <td>-0.470212</td>\n",
       "      <td>-0.272881</td>\n",
       "      <td>-0.219902</td>\n",
       "      <td>-0.206578</td>\n",
       "      <td>-0.180400</td>\n",
       "      <td>-0.062087</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11607183</th>\n",
       "      <td>-1.23</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.70</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.552156e+08</td>\n",
       "      <td>255.024338</td>\n",
       "      <td>...</td>\n",
       "      <td>1098.721924</td>\n",
       "      <td>2879.136963</td>\n",
       "      <td>-0.470212</td>\n",
       "      <td>-0.283529</td>\n",
       "      <td>-0.225654</td>\n",
       "      <td>-0.218636</td>\n",
       "      <td>-0.192258</td>\n",
       "      <td>-0.062087</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11607184</th>\n",
       "      <td>-1.27</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.02</td>\n",
       "      <td>3.85</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.552156e+08</td>\n",
       "      <td>255.024338</td>\n",
       "      <td>...</td>\n",
       "      <td>1065.043213</td>\n",
       "      <td>2619.544189</td>\n",
       "      <td>-0.488253</td>\n",
       "      <td>-0.283529</td>\n",
       "      <td>-0.219902</td>\n",
       "      <td>-0.206578</td>\n",
       "      <td>-0.180400</td>\n",
       "      <td>-0.042108</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11607185</th>\n",
       "      <td>-1.16</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.81</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.552156e+08</td>\n",
       "      <td>253.656616</td>\n",
       "      <td>...</td>\n",
       "      <td>965.210388</td>\n",
       "      <td>2381.172363</td>\n",
       "      <td>-0.438639</td>\n",
       "      <td>-0.262233</td>\n",
       "      <td>-0.212232</td>\n",
       "      <td>-0.199688</td>\n",
       "      <td>-0.175953</td>\n",
       "      <td>-0.047435</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11607186</th>\n",
       "      <td>-1.23</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.74</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.552156e+08</td>\n",
       "      <td>255.173065</td>\n",
       "      <td>...</td>\n",
       "      <td>1122.912476</td>\n",
       "      <td>2977.040283</td>\n",
       "      <td>-0.470212</td>\n",
       "      <td>-0.283529</td>\n",
       "      <td>-0.225654</td>\n",
       "      <td>-0.213468</td>\n",
       "      <td>-0.192258</td>\n",
       "      <td>-0.056759</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11607187 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          RH_25  RH_50  RH_75  RH_85  RH_95  RH_100  channel  degrade_flag  \\\n",
       "0         -0.78   0.00   0.78   1.19   1.79    2.36        0             0   \n",
       "1         -0.97  -0.07   0.78   1.23   1.87    2.54        0             0   \n",
       "2         -0.86  -0.07   0.71   1.12   1.72    2.32        0             0   \n",
       "3         -0.86  -0.07   0.74   1.16   1.79    2.47        0             0   \n",
       "4         -0.89  -0.03   0.78   1.23   1.94    2.73        0             0   \n",
       "...         ...    ...    ...    ...    ...     ...      ...           ...   \n",
       "11607182  -1.23  -0.18   0.74   1.23   2.02    3.70        5             0   \n",
       "11607183  -1.23  -0.22   0.71   1.16   1.94    3.70        5             0   \n",
       "11607184  -1.27  -0.22   0.74   1.23   2.02    3.85        5             0   \n",
       "11607185  -1.16  -0.14   0.78   1.27   2.05    3.81        5             0   \n",
       "11607186  -1.23  -0.22   0.71   1.19   1.94    3.74        5             0   \n",
       "\n",
       "            delta_time  digital_elevation_model  ...   zcross_amp  \\\n",
       "0         4.146742e+07           -999999.000000  ...   296.142761   \n",
       "1         4.146742e+07           -999999.000000  ...   302.973358   \n",
       "2         4.146742e+07           -999999.000000  ...   302.514313   \n",
       "3         4.146742e+07           -999999.000000  ...   309.622925   \n",
       "4         4.146742e+07           -999999.000000  ...   320.783600   \n",
       "...                ...                      ...  ...          ...   \n",
       "11607182  1.552156e+08               254.929123  ...  1057.588623   \n",
       "11607183  1.552156e+08               255.024338  ...  1098.721924   \n",
       "11607184  1.552156e+08               255.024338  ...  1065.043213   \n",
       "11607185  1.552156e+08               253.656616  ...   965.210388   \n",
       "11607186  1.552156e+08               255.173065  ...  1122.912476   \n",
       "\n",
       "          zcross_localenergy  RH_25 Z Score  RH_50 Z Score  RH_75 Z Score  \\\n",
       "0                 163.479019      -0.267243      -0.224966      -0.212232   \n",
       "1                 178.583328      -0.352941      -0.243600      -0.212232   \n",
       "2                 203.563889      -0.303326      -0.243600      -0.225654   \n",
       "3                 221.641510      -0.303326      -0.243600      -0.219902   \n",
       "4                 268.898804      -0.316857      -0.232952      -0.212232   \n",
       "...                      ...            ...            ...            ...   \n",
       "11607182         2616.521729      -0.470212      -0.272881      -0.219902   \n",
       "11607183         2879.136963      -0.470212      -0.283529      -0.225654   \n",
       "11607184         2619.544189      -0.488253      -0.283529      -0.219902   \n",
       "11607185         2381.172363      -0.438639      -0.262233      -0.212232   \n",
       "11607186         2977.040283      -0.470212      -0.283529      -0.225654   \n",
       "\n",
       "          RH_85 Z Score  RH_95 Z Score  RH_100 Z Score  RH_95_minus_50  \\\n",
       "0             -0.213468      -0.214492       -0.240572            1.79   \n",
       "1             -0.206578      -0.202634       -0.216597            1.94   \n",
       "2             -0.225526      -0.224867       -0.245900            1.79   \n",
       "3             -0.218636      -0.214492       -0.225921            1.86   \n",
       "4             -0.206578      -0.192258       -0.191289            1.97   \n",
       "...                 ...            ...             ...             ...   \n",
       "11607182      -0.206578      -0.180400       -0.062087            2.20   \n",
       "11607183      -0.218636      -0.192258       -0.062087            2.16   \n",
       "11607184      -0.206578      -0.180400       -0.042108            2.24   \n",
       "11607185      -0.199688      -0.175953       -0.047435            2.19   \n",
       "11607186      -0.213468      -0.192258       -0.056759            2.16   \n",
       "\n",
       "          Missingness  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "11607182            0  \n",
       "11607183            0  \n",
       "11607184            0  \n",
       "11607185            0  \n",
       "11607186            0  \n",
       "\n",
       "[11607187 rows x 85 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(f'Original dataframe shape: {complete_df.shape}')\n",
    "\n",
    "'''Filtering'''\n",
    "complete_df.drop('Shot Number', axis=1, inplace=True)\n",
    "print(\"Shot numbers dropped\")\n",
    "discounted_df = complete_df.dropna(axis=1)\n",
    "\n",
    "\n",
    "# imputer = SimpleImputer(strategy='most_frequent')  # You can change to 'median', 'most_frequent', etc.\n",
    "# discounted_df = pd.DataFrame(imputer.fit_transform(complete_df), columns=complete_df.columns)\n",
    "\n",
    "print(\"Dataframe shape after dropping NaN columns:\", discounted_df.shape)\n",
    "\n",
    "# Step 1: Separate features and target (if applicable)\n",
    "# Exclude non-numeric columns if present\n",
    "filtered_df = discounted_df.select_dtypes(include=[np.number])\n",
    "# columns_to_keep = [col for col in numeric_df.columns if 'RH' not in col]\n",
    "# filtered_df = filtered_df[columns_to_keep]\n",
    "\n",
    "print(f'Non-numeric columns removed from dataframe\\nCleaned dataframe size: {filtered_df.shape}')\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97056d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made new scaled dataframe\n",
      "Successfully wrote to a .parquet file!\n"
     ]
    }
   ],
   "source": [
    "# Step 2, OPTION 1: Standardize the data using StandardScalar (for demo only)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaled_data = scaler.fit_transform(filtered_df)\n",
    "# print(f'Data scaled\\nScaled data size (ndarray): {scaled_data.shape}')\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=filtered_df.columns, index=filtered_df.index)\n",
    "print(\"Made new scaled dataframe\")\n",
    "scaled_df\n",
    "scaled_df.describe()\n",
    "\n",
    "# Uncomment the two lines below to write scaled_df to parquet\n",
    "scaled_df.to_parquet(\"input_standard_scaled.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "print('Successfully wrote to a .parquet file!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81a602a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated RobustScalar\n",
      "Fitted RobustScalar\n",
      "Made new scaled dataframe\n",
      "Successfully wrote to a .parquet file!\n"
     ]
    }
   ],
   "source": [
    "# OPTION 2: Standardize the data using RobustScalar (this is better for outlier detection)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Assume `filtered_df` is your cleaned and filtered dataframe (all numeric columns)\n",
    "robust_scaler = RobustScaler()\n",
    "print(\"Instantiated RobustScalar\")\n",
    "\n",
    "# Fit the scaler on the dataframe and transform it\n",
    "scaled_array = robust_scaler.fit_transform(filtered_df)\n",
    "print(\"Fitted RobustScalar\")\n",
    "\n",
    "# (Optional) Create a new DataFrame with scaled values\n",
    "scaled_df = pd.DataFrame(scaled_array, columns=filtered_df.columns, index=filtered_df.index)\n",
    "print(\"Made new scaled dataframe\")\n",
    "scaled_df\n",
    "scaled_df.describe()\n",
    "\n",
    "# Uncomment the two lines below to write scaled_df to parquet\n",
    "scaled_df.to_parquet(\"input_scaled.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "print('Successfully wrote to a .parquet file!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEDI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
