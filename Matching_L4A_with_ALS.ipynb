{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43779e86e13e3b84",
   "metadata": {},
   "source": [
    "# Matching shot_number between GEDI L4A & ALS\n",
    "We will match shot numbers in the Level 2A GEDI data with the ALS crossover data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:12.404923Z",
     "start_time": "2024-12-03T16:42:12.088600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "# import geoviews as gv\n",
    "# from geoviews import opts, tile_sources as gvts\n",
    "# import holoviews as hv\n",
    "# gv.extension('bokeh', 'matplotlib')\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# import PyQt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abfb1225ff906d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:12.916873Z",
     "start_time": "2024-12-03T16:42:12.903957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/oscar/scratch/jzhu118/GEDI_Outlier_Detection_OSCAR/Input_files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['GEDI04_A_2021009022644_O11762_03_T01637_02_002_02_V002.h5',\n",
       " 'GEDI04_A_2022106075705_O18927_04_T10647_02_003_01_V002.h5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inDir = os.getcwd() + \"/Input_files\"\n",
    "print(inDir)\n",
    "input_file_names = [g for g in os.listdir(inDir) if g.startswith('GEDI04_A') and g.endswith('.h5')]  # List all GEDI level 2 files in inDir\n",
    "input_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193cf3a589257efc",
   "metadata": {},
   "source": [
    "### Loading files with all information into a huge Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ca866",
   "metadata": {},
   "source": [
    "#### Preprocessing: Get files and sort by beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a37290db29772d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:13.026872Z",
     "start_time": "2024-12-03T16:42:12.947179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: GEDI04_A_2021009022644_O11762_03_T01637_02_002_02_V002.h5\n",
      "The file contains the following groups: ['ANCILLARY', 'BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011', 'METADATA']\n",
      "The file's metadata contains the following attributes: \n",
      "PGEVersion\n",
      "VersionID\n",
      "abstract\n",
      "characterSet\n",
      "creationDate\n",
      "credit\n",
      "fileName\n",
      "gedi_l4a_githash\n",
      "language\n",
      "originatorOrganizationName\n",
      "purpose\n",
      "shortName\n",
      "spatialRepresentationType\n",
      "status\n",
      "topicCategory\n",
      "uuid\n",
      "The file contains the following beams: \n",
      "BEAM0000 is a Coverage beam\n",
      "BEAM0001 is a Coverage beam\n",
      "BEAM0010 is a Coverage beam\n",
      "BEAM0011 is a Coverage beam\n",
      "BEAM0101 is a Full power beam\n",
      "BEAM0110 is a Full power beam\n",
      "BEAM1000 is a Full power beam\n",
      "BEAM1011 is a Full power beam\n",
      "Loading file: GEDI04_A_2022106075705_O18927_04_T10647_02_003_01_V002.h5\n",
      "The file contains the following groups: ['ANCILLARY', 'BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011', 'METADATA']\n",
      "The file's metadata contains the following attributes: \n",
      "PGEVersion\n",
      "VersionID\n",
      "abstract\n",
      "characterSet\n",
      "creationDate\n",
      "credit\n",
      "fileName\n",
      "gedi_l4a_githash\n",
      "language\n",
      "originatorOrganizationName\n",
      "purpose\n",
      "shortName\n",
      "spatialRepresentationType\n",
      "status\n",
      "topicCategory\n",
      "uuid\n",
      "The file contains the following beams: \n",
      "BEAM0000 is a Coverage beam\n",
      "BEAM0001 is a Coverage beam\n",
      "BEAM0010 is a Coverage beam\n",
      "BEAM0011 is a Coverage beam\n",
      "BEAM0101 is a Full power beam\n",
      "BEAM0110 is a Full power beam\n",
      "BEAM1000 is a Full power beam\n",
      "BEAM1011 is a Full power beam\n"
     ]
    }
   ],
   "source": [
    "input_files = []\n",
    "files_to_beams = dict()\n",
    "for n in input_file_names:\n",
    "    file_path = os.path.join(inDir, n)  # Select an example file\n",
    "    file = h5py.File(file_path, 'r')\n",
    "    input_files.append(file)\n",
    "    \n",
    "    print('Loading file: ' + n)\n",
    "    print('The file contains the following groups: ' + str(list(file.keys())))\n",
    "    \n",
    "    print(\"The file's metadata contains the following attributes: \")\n",
    "    for g in file['METADATA']['DatasetIdentification'].attrs: print(g)\n",
    "    \n",
    "    beamNames = [g for g in file.keys() if g.startswith('BEAM')]\n",
    "    files_to_beams[file] = beamNames\n",
    "    \n",
    "    print(\"The file contains the following beams: \")\n",
    "    for b in beamNames:\n",
    "        print(f\"{b} is a {file[b].attrs['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14096b",
   "metadata": {},
   "source": [
    "#### Helper functions for the main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660146583f485f77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:14.006557Z",
     "start_time": "2024-12-03T16:42:13.992165Z"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def collect_all_datasets(file_path):\n",
    "    \"\"\"\n",
    "    Recursively collects all dataset paths within the HDF5 file.\n",
    "    Returns a list of dataset path strings.\n",
    "    \"\"\"\n",
    "    dataset_paths = []\n",
    "    \n",
    "    def visitor_func(name, node):\n",
    "        if isinstance(node, h5py.Dataset):\n",
    "            dataset_paths.append(name)\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        f.visititems(visitor_func)\n",
    "    \n",
    "    return dataset_paths\n",
    "\n",
    "def get_dataset_by_name(h5_file, beam_ds, name):\n",
    "    \"\"\"\n",
    "    Given an open HDF5 file (h5_file) and a list of dataset paths (beam_ds),\n",
    "    returns the data for the first dataset whose path ends with '/{name}'.\n",
    "    If not found, returns None.\n",
    "    \"\"\"\n",
    "    candidates = [ds for ds in beam_ds if ds.endswith(f'/{name}')]\n",
    "    if not candidates:\n",
    "        print(f\"Warning: No dataset ending with '/{name}' found.\")\n",
    "        return None\n",
    "    dataset_path = candidates[0]\n",
    "    return h5_file[dataset_path][()]  # Read dataset into memory\n",
    "\n",
    "def load_attributes_to_dict(txt_file_path):\n",
    "    \"\"\"\n",
    "    Reads attribute names from a text file (one per line) and returns a dictionary.\n",
    "    This dictionary maps the human-friendly column name to the dataset suffix.\n",
    "    \"\"\"\n",
    "    with open(txt_file_path, 'r') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    attr_dict = {}\n",
    "    for raw_attr in lines:\n",
    "        attr_dict[raw_attr] = raw_attr  # Direct mapping for simplicity\n",
    "    return attr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a951e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ALS crossover data with 76778 unique shot numbers.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# LOAD ALS CROSSOVER DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "als_csv_path = os.path.join('Input_files', 'GEDI_ALSCROSSOVERS.csv')\n",
    "als_df = pd.read_csv(als_csv_path, dtype={'shot_number': str})\n",
    "\n",
    "# Ensure shot numbers are of a consistent type (adjust if necessary)\n",
    "als_df['shot_number'] = als_df['shot_number'].astype(str)\n",
    "als_shot_set = set(als_df['shot_number'])\n",
    "print(f\"Loaded ALS crossover data with {len(als_shot_set)} unique shot numbers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b7aa9",
   "metadata": {},
   "source": [
    "### THIS CELL IS FOR TESTING ONLY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e948bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 233736 shot numbers for als_shot_set.\n"
     ]
    }
   ],
   "source": [
    "# Read the parquet file containing all GEDI waveform shots\n",
    "gedi_df = pd.read_parquet('L4A_raw.parquet')\n",
    "\n",
    "# Assume the shot numbers are in a column named 'shot_number'\n",
    "# Randomly sample 10% of the shot numbers for testing purposes.\n",
    "# Set a random state for reproducibility.\n",
    "sample_fraction = 0.1\n",
    "random_state = 42\n",
    "\n",
    "als_shot_series = gedi_df['Shot Number'].sample(frac=sample_fraction, random_state=random_state)\n",
    "\n",
    "# Convert shot numbers to strings for consistent matching later on\n",
    "als_shot_set = set(als_shot_series.astype(str))\n",
    "\n",
    "print(f\"Selected {len(als_shot_set)} shot numbers for als_shot_set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d7a3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded attributes: {'agbd': 'agbd', 'agbd_a1': 'agbd_a1', 'agbd_a10': 'agbd_a10', 'agbd_a2': 'agbd_a2', 'agbd_a3': 'agbd_a3', 'agbd_a4': 'agbd_a4', 'agbd_a5': 'agbd_a5', 'agbd_a6': 'agbd_a6', 'agbd_pi_lower': 'agbd_pi_lower', 'agbd_pi_lower_a1': 'agbd_pi_lower_a1', 'agbd_pi_lower_a10': 'agbd_pi_lower_a10', 'agbd_pi_lower_a2': 'agbd_pi_lower_a2', 'agbd_pi_lower_a3': 'agbd_pi_lower_a3', 'agbd_pi_lower_a4': 'agbd_pi_lower_a4', 'agbd_pi_lower_a5': 'agbd_pi_lower_a5', 'agbd_pi_lower_a6': 'agbd_pi_lower_a6', 'agbd_pi_upper': 'agbd_pi_upper', 'agbd_pi_upper_a1': 'agbd_pi_upper_a1', 'agbd_pi_upper_a10': 'agbd_pi_upper_a10', 'agbd_pi_upper_a2': 'agbd_pi_upper_a2', 'agbd_pi_upper_a3': 'agbd_pi_upper_a3', 'agbd_pi_upper_a4': 'agbd_pi_upper_a4', 'agbd_pi_upper_a5': 'agbd_pi_upper_a5', 'agbd_pi_upper_a6': 'agbd_pi_upper_a6', 'agbd_se': 'agbd_se', 'agbd_se_a1': 'agbd_se_a1', 'agbd_se_a10': 'agbd_se_a10', 'agbd_se_a2': 'agbd_se_a2', 'agbd_se_a3': 'agbd_se_a3', 'agbd_se_a4': 'agbd_se_a4', 'agbd_se_a5': 'agbd_se_a5', 'agbd_se_a6': 'agbd_se_a6', 'agbd_t': 'agbd_t', 'agbd_t_a1': 'agbd_t_a1', 'agbd_t_a10': 'agbd_t_a10', 'agbd_t_a2': 'agbd_t_a2', 'agbd_t_a3': 'agbd_t_a3', 'agbd_t_a4': 'agbd_t_a4', 'agbd_t_a5': 'agbd_t_a5', 'agbd_t_a6': 'agbd_t_a6', 'agbd_t_pi_lower_a1': 'agbd_t_pi_lower_a1', 'agbd_t_pi_lower_a10': 'agbd_t_pi_lower_a10', 'agbd_t_pi_lower_a2': 'agbd_t_pi_lower_a2', 'agbd_t_pi_lower_a3': 'agbd_t_pi_lower_a3', 'agbd_t_pi_lower_a4': 'agbd_t_pi_lower_a4', 'agbd_t_pi_lower_a5': 'agbd_t_pi_lower_a5', 'agbd_t_pi_lower_a6': 'agbd_t_pi_lower_a6', 'agbd_t_pi_upper_a1': 'agbd_t_pi_upper_a1', 'agbd_t_pi_upper_a10': 'agbd_t_pi_upper_a10', 'agbd_t_pi_upper_a2': 'agbd_t_pi_upper_a2', 'agbd_t_pi_upper_a3': 'agbd_t_pi_upper_a3', 'agbd_t_pi_upper_a4': 'agbd_t_pi_upper_a4', 'agbd_t_pi_upper_a5': 'agbd_t_pi_upper_a5', 'agbd_t_pi_upper_a6': 'agbd_t_pi_upper_a6', 'agbd_t_se': 'agbd_t_se', 'agbd_t_se_a1': 'agbd_t_se_a1', 'agbd_t_se_a10': 'agbd_t_se_a10', 'agbd_t_se_a2': 'agbd_t_se_a2', 'agbd_t_se_a3': 'agbd_t_se_a3', 'agbd_t_se_a4': 'agbd_t_se_a4', 'agbd_t_se_a5': 'agbd_t_se_a5', 'agbd_t_se_a6': 'agbd_t_se_a6', 'algorithm_run_flag': 'algorithm_run_flag', 'algorithm_run_flag_a1': 'algorithm_run_flag_a1', 'algorithm_run_flag_a10': 'algorithm_run_flag_a10', 'algorithm_run_flag_a2': 'algorithm_run_flag_a2', 'algorithm_run_flag_a3': 'algorithm_run_flag_a3', 'algorithm_run_flag_a4': 'algorithm_run_flag_a4', 'algorithm_run_flag_a5': 'algorithm_run_flag_a5', 'algorithm_run_flag_a6': 'algorithm_run_flag_a6', 'beam': 'beam', 'channel': 'channel', 'degrade_flag': 'degrade_flag', 'delta_time': 'delta_time', 'elev_lowestmode': 'elev_lowestmode', 'elev_lowestmode_a1': 'elev_lowestmode_a1', 'elev_lowestmode_a10': 'elev_lowestmode_a10', 'elev_lowestmode_a2': 'elev_lowestmode_a2', 'elev_lowestmode_a3': 'elev_lowestmode_a3', 'elev_lowestmode_a4': 'elev_lowestmode_a4', 'elev_lowestmode_a5': 'elev_lowestmode_a5', 'elev_lowestmode_a6': 'elev_lowestmode_a6', 'l2_quality_flag': 'l2_quality_flag', 'l2_quality_flag_a1': 'l2_quality_flag_a1', 'l2_quality_flag_a10': 'l2_quality_flag_a10', 'l2_quality_flag_a2': 'l2_quality_flag_a2', 'l2_quality_flag_a3': 'l2_quality_flag_a3', 'l2_quality_flag_a4': 'l2_quality_flag_a4', 'l2_quality_flag_a5': 'l2_quality_flag_a5', 'l2_quality_flag_a6': 'l2_quality_flag_a6', 'l4_quality_flag': 'l4_quality_flag', 'l4_quality_flag_a1': 'l4_quality_flag_a1', 'l4_quality_flag_a10': 'l4_quality_flag_a10', 'l4_quality_flag_a2': 'l4_quality_flag_a2', 'l4_quality_flag_a3': 'l4_quality_flag_a3', 'l4_quality_flag_a4': 'l4_quality_flag_a4', 'l4_quality_flag_a5': 'l4_quality_flag_a5', 'l4_quality_flag_a6': 'l4_quality_flag_a6', 'landsat_treecover': 'landsat_treecover', 'landsat_water_persistence': 'landsat_water_persistence', 'lat_lowestmode': 'lat_lowestmode', 'lat_lowestmode_a1': 'lat_lowestmode_a1', 'lat_lowestmode_a10': 'lat_lowestmode_a10', 'lat_lowestmode_a2': 'lat_lowestmode_a2', 'lat_lowestmode_a3': 'lat_lowestmode_a3', 'lat_lowestmode_a4': 'lat_lowestmode_a4', 'lat_lowestmode_a5': 'lat_lowestmode_a5', 'lat_lowestmode_a6': 'lat_lowestmode_a6', 'leaf_off_doy': 'leaf_off_doy', 'leaf_off_flag': 'leaf_off_flag', 'leaf_on_cycle': 'leaf_on_cycle', 'leaf_on_doy': 'leaf_on_doy', 'lon_lowestmode': 'lon_lowestmode', 'lon_lowestmode_a1': 'lon_lowestmode_a1', 'lon_lowestmode_a10': 'lon_lowestmode_a10', 'lon_lowestmode_a2': 'lon_lowestmode_a2', 'lon_lowestmode_a3': 'lon_lowestmode_a3', 'lon_lowestmode_a4': 'lon_lowestmode_a4', 'lon_lowestmode_a5': 'lon_lowestmode_a5', 'lon_lowestmode_a6': 'lon_lowestmode_a6', 'master_frac': 'master_frac', 'master_int': 'master_int', 'pft_class': 'pft_class', 'predict_stratum': 'predict_stratum', 'predictor_limit_flag': 'predictor_limit_flag', 'predictor_limit_flag_a1': 'predictor_limit_flag_a1', 'predictor_limit_flag_a10': 'predictor_limit_flag_a10', 'predictor_limit_flag_a2': 'predictor_limit_flag_a2', 'predictor_limit_flag_a3': 'predictor_limit_flag_a3', 'predictor_limit_flag_a4': 'predictor_limit_flag_a4', 'predictor_limit_flag_a5': 'predictor_limit_flag_a5', 'predictor_limit_flag_a6': 'predictor_limit_flag_a6', 'region_class': 'region_class', 'response_limit_flag': 'response_limit_flag', 'response_limit_flag_a1': 'response_limit_flag_a1', 'response_limit_flag_a10': 'response_limit_flag_a10', 'response_limit_flag_a2': 'response_limit_flag_a2', 'response_limit_flag_a3': 'response_limit_flag_a3', 'response_limit_flag_a4': 'response_limit_flag_a4', 'response_limit_flag_a5': 'response_limit_flag_a5', 'response_limit_flag_a6': 'response_limit_flag_a6', 'selected_algorithm': 'selected_algorithm', 'selected_mode': 'selected_mode', 'selected_mode_a1': 'selected_mode_a1', 'selected_mode_a10': 'selected_mode_a10', 'selected_mode_a2': 'selected_mode_a2', 'selected_mode_a3': 'selected_mode_a3', 'selected_mode_a4': 'selected_mode_a4', 'selected_mode_a5': 'selected_mode_a5', 'selected_mode_a6': 'selected_mode_a6', 'selected_mode_flag': 'selected_mode_flag', 'selected_mode_flag_a1': 'selected_mode_flag_a1', 'selected_mode_flag_a10': 'selected_mode_flag_a10', 'selected_mode_flag_a2': 'selected_mode_flag_a2', 'selected_mode_flag_a3': 'selected_mode_flag_a3', 'selected_mode_flag_a4': 'selected_mode_flag_a4', 'selected_mode_flag_a5': 'selected_mode_flag_a5', 'selected_mode_flag_a6': 'selected_mode_flag_a6', 'sensitivity': 'sensitivity', 'sensitivity_a1': 'sensitivity_a1', 'sensitivity_a10': 'sensitivity_a10', 'sensitivity_a2': 'sensitivity_a2', 'sensitivity_a3': 'sensitivity_a3', 'sensitivity_a4': 'sensitivity_a4', 'sensitivity_a6': 'sensitivity_a6', 'shot_number': 'shot_number', 'solar_elevation': 'solar_elevation', 'stale_return_flag': 'stale_return_flag', 'surface_flag': 'surface_flag', 'urban_focal_window_size': 'urban_focal_window_size', 'urban_proportion': 'urban_proportion'}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# LOAD ATTRIBUTE DICTIONARY (including Beam and Channel, among others)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "txt_path = 'L4A_features.txt'\n",
    "ATTRIBUTES_TO_LOAD = load_attributes_to_dict(txt_path)\n",
    "print(\"Loaded attributes:\", ATTRIBUTES_TO_LOAD)\n",
    "\n",
    "# Define the list of xvar metrics to extract (each is 2D with shape (# shots, 4))\n",
    "xvar_features = ['xvar', 'xvar_a1', 'xvar_a2', 'xvar_a3', \n",
    "                 'xvar_a4', 'xvar_a5', 'xvar_a6', 'xvar_a10']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4574d5",
   "metadata": {},
   "source": [
    "#### Main loop, collects all target features and puts them inside a Pandas dataframe.\n",
    "WARNING: Despite my best attempts at optimization, this is *extremely* memory intensive even for one 5GB HDF5 file. Only run on a supercomputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe575445c09a1d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:50:27.138768Z",
     "start_time": "2024-12-03T17:37:18.265405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing L4A file: /oscar/scratch/jzhu118/GEDI_Outlier_Detection_OSCAR/Input_files/GEDI04_A_2021009022644_O11762_03_T01637_02_002_02_V002.h5\n",
      "  Processing beam: BEAM0000\n",
      "    Total number of shots: 167298\n",
      "    Found 16615 matching shots.\n",
      "    Processed beam BEAM0000 with 16615 matching shots.\n",
      "  Processing beam: BEAM0001\n",
      "    Total number of shots: 167288\n",
      "    Found 16882 matching shots.\n",
      "    Processed beam BEAM0001 with 16882 matching shots.\n",
      "  Processing beam: BEAM0010\n",
      "    Total number of shots: 167310\n",
      "    Found 16855 matching shots.\n",
      "    Processed beam BEAM0010 with 16855 matching shots.\n",
      "  Processing beam: BEAM0011\n",
      "    Total number of shots: 167328\n",
      "    Found 16633 matching shots.\n",
      "    Processed beam BEAM0011 with 16633 matching shots.\n",
      "  Processing beam: BEAM0101\n",
      "    Total number of shots: 167265\n",
      "    Found 16831 matching shots.\n",
      "    Processed beam BEAM0101 with 16831 matching shots.\n",
      "  Processing beam: BEAM0110\n",
      "    Total number of shots: 167030\n",
      "    Found 16633 matching shots.\n",
      "    Processed beam BEAM0110 with 16633 matching shots.\n",
      "  Processing beam: BEAM1000\n",
      "    Total number of shots: 166838\n",
      "    Found 16678 matching shots.\n",
      "    Processed beam BEAM1000 with 16678 matching shots.\n",
      "  Processing beam: BEAM1011\n",
      "    Total number of shots: 167113\n",
      "    Found 16596 matching shots.\n",
      "    Processed beam BEAM1011 with 16596 matching shots.\n",
      "\n",
      "Processing L4A file: /oscar/scratch/jzhu118/GEDI_Outlier_Detection_OSCAR/Input_files/GEDI04_A_2022106075705_O18927_04_T10647_02_003_01_V002.h5\n",
      "  Processing beam: BEAM0000\n",
      "    Total number of shots: 125482\n",
      "    Found 12694 matching shots.\n",
      "    Processed beam BEAM0000 with 12694 matching shots.\n",
      "  Processing beam: BEAM0001\n",
      "    Total number of shots: 124405\n",
      "    Found 12320 matching shots.\n",
      "    Processed beam BEAM0001 with 12320 matching shots.\n",
      "  Processing beam: BEAM0010\n",
      "    Total number of shots: 125473\n",
      "    Found 12530 matching shots.\n",
      "    Processed beam BEAM0010 with 12530 matching shots.\n",
      "  Processing beam: BEAM0011\n",
      "    Total number of shots: 124398\n",
      "    Found 12597 matching shots.\n",
      "    Processed beam BEAM0011 with 12597 matching shots.\n",
      "  Processing beam: BEAM0101\n",
      "    Total number of shots: 124514\n",
      "    Found 12432 matching shots.\n",
      "    Processed beam BEAM0101 with 12432 matching shots.\n",
      "  Processing beam: BEAM0110\n",
      "    Total number of shots: 125318\n",
      "    Found 12421 matching shots.\n",
      "    Processed beam BEAM0110 with 12421 matching shots.\n",
      "  Processing beam: BEAM1000\n",
      "    Total number of shots: 125723\n",
      "    Found 12551 matching shots.\n",
      "    Processed beam BEAM1000 with 12551 matching shots.\n",
      "  Processing beam: BEAM1011\n",
      "    Total number of shots: 124578\n",
      "    Found 12468 matching shots.\n",
      "    Processed beam BEAM1011 with 12468 matching shots.\n",
      "Encoding complete\n",
      "\n",
      "Final DataFrame shape: (233736, 231)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# MAIN EXTRACTION LOOP FOR L4A FILES WITH ALS MATCHING\n",
    "# -------------------------\n",
    "\n",
    "# List to hold DataFrames for each beam\n",
    "dataframes = []\n",
    "\n",
    "for f in input_files:\n",
    "    print(f\"\\nProcessing L4A file: {f.filename}\")\n",
    "    \n",
    "    # Collect all dataset paths in the current file\n",
    "    all_ds_for_file = collect_all_datasets(f.filename)\n",
    "    \n",
    "    # Process each beam in the file\n",
    "    for b in files_to_beams[f]:\n",
    "        print(f\"  Processing beam: {b}\")\n",
    "        \n",
    "        # Filter dataset paths to only those belonging to the current beam\n",
    "        beam_ds = [ds for ds in all_ds_for_file if b in ds]\n",
    "        \n",
    "        # 1) Retrieve the shot_number dataset (1D array; one entry per shot)\n",
    "        shotNums = get_dataset_by_name(f, beam_ds, 'shot_number')\n",
    "        if shotNums is None:\n",
    "            print(f\"    Warning: No 'shot_number' dataset found for beam {b}. Skipping beam.\")\n",
    "            continue\n",
    "        \n",
    "        # Convert shot numbers to a NumPy array of strings (for consistent matching)\n",
    "        shotNums = np.array(shotNums).astype(str)\n",
    "        total_shots = len(shotNums)\n",
    "        print(f\"    Total number of shots: {total_shots}\")\n",
    "        \n",
    "        # --- MATCHING WITH ALS DATA ---\n",
    "        # Determine matching indices using the als_shot_set\n",
    "        matching_indices = np.where(np.isin(shotNums, list(als_shot_set)))[0]\n",
    "        if matching_indices.size == 0:\n",
    "            print(f\"    No matching shot numbers found for beam {b}. Skipping beam.\")\n",
    "            continue\n",
    "        filtered_shot_count = matching_indices.size\n",
    "        print(f\"    Found {filtered_shot_count} matching shots.\")\n",
    "        \n",
    "        # Filter shot numbers and build constant columns for the matching shots only\n",
    "        shotNums_filtered = shotNums[matching_indices]\n",
    "        file_name_col = [os.path.basename(f.filename)] * filtered_shot_count\n",
    "        beam_name_col = [b] * filtered_shot_count\n",
    "        \n",
    "        # 2) Extract all 1D features specified in the L4A feature list, filtering by matching indices\n",
    "        attr_data = {}\n",
    "        for col_label, ds_suffix in ATTRIBUTES_TO_LOAD.items():\n",
    "            data_array = get_dataset_by_name(f, beam_ds, ds_suffix)\n",
    "            if data_array is None:\n",
    "                print(f\"    Warning: Missing dataset for {ds_suffix}. Filling with None.\")\n",
    "                attr_data[col_label] = np.full(total_shots, None)[matching_indices]\n",
    "            else:\n",
    "                data_array = np.array(data_array)\n",
    "                if len(data_array) != total_shots:\n",
    "                    print(f\"    Warning: Row count mismatch for {col_label} in beam {b}.\")\n",
    "                # Filter to keep only matching shots\n",
    "                attr_data[col_label] = data_array[matching_indices]\n",
    "        \n",
    "        # 3) Extract and process each xvar feature (each is 2D with shape (# shots, 4)), filtering by matching indices\n",
    "        xvar_data = {}\n",
    "        for feature in xvar_features:\n",
    "            data_array = get_dataset_by_name(f, beam_ds, feature)\n",
    "            if data_array is None:\n",
    "                print(f\"    Warning: Missing xvar feature: {feature}. Filling with NaNs.\")\n",
    "                for i in range(1, 5):\n",
    "                    xvar_data[f\"{feature}_{i}\"] = np.full(total_shots, np.nan)[matching_indices]\n",
    "            else:\n",
    "                data_array = np.array(data_array)\n",
    "                # Check if transposition is needed: expected shape is (total_shots, 4)\n",
    "                if data_array.shape[0] == 4 and data_array.shape[1] == total_shots:\n",
    "                    data_array = data_array.T\n",
    "                elif data_array.shape[0] != total_shots or data_array.shape[1] != 4:\n",
    "                    print(f\"    Warning: Unexpected shape for {feature}. Expected (# shots, 4). Got {data_array.shape}.\")\n",
    "                # Now, for each of the 4 columns, filter by matching_indices\n",
    "                for i in range(4):\n",
    "                    col_name = f\"{feature}_{i+1}\"\n",
    "                    xvar_data[col_name] = data_array[:, i][matching_indices]\n",
    "        \n",
    "        # 4) Construct the DataFrame for the current beam by combining all columns for matching shots\n",
    "        df = pd.DataFrame({\n",
    "            'File Name': file_name_col,\n",
    "            'Beam Name': beam_name_col,\n",
    "            'Shot Number': shotNums_filtered,\n",
    "            **attr_data,\n",
    "            **xvar_data\n",
    "        })\n",
    "        \n",
    "        dataframes.append(df)\n",
    "        print(f\"    Processed beam {b} with {filtered_shot_count} matching shots.\")\n",
    "\n",
    "# Optionally, concatenate all beam DataFrames into one complete DataFrame:\n",
    "if dataframes:\n",
    "    complete_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # 5) One-hot encode categorical features, e.g., Beam Name and channel (if present)\n",
    "    if 'beam' in complete_df.columns:\n",
    "        complete_df = pd.get_dummies(complete_df, columns=['beam'], prefix='beam')\n",
    "    if 'channel' in complete_df.columns:\n",
    "        complete_df = pd.get_dummies(complete_df, columns=['channel'], prefix='channel')\n",
    "    if 'region_class' in complete_df.columns:\n",
    "        complete_df = pd.get_dummies(complete_df, columns=['region_class'], prefix='region')\n",
    "    if 'pft_class' in complete_df.columns:\n",
    "        complete_df = pd.get_dummies(complete_df, columns=['pft_class'], prefix='pft')\n",
    "    print('Encoding complete')\n",
    "\n",
    "    print(f\"\\nFinal DataFrame shape: {complete_df.shape}\")\n",
    "else:\n",
    "    complete_df = pd.DataFrame()\n",
    "    print(\"\\nNo data extracted from L4A files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbdb9ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate columns: Index([], dtype='object')\n",
      "There are 0 duplicated columns.\n",
      "NaN value count:\n",
      "\n",
      "File Name             0\n",
      "Shot Number           0\n",
      "agbd                  0\n",
      "agbd_a1               0\n",
      "agbd_a10              0\n",
      "                  ...  \n",
      "channel_3        204682\n",
      "Beam_BEAM1000    204507\n",
      "channel_4        204507\n",
      "Beam_BEAM1011    204672\n",
      "channel_5        204672\n",
      "Length: 218, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agbd</th>\n",
       "      <th>agbd_a1</th>\n",
       "      <th>agbd_a10</th>\n",
       "      <th>agbd_a2</th>\n",
       "      <th>agbd_a3</th>\n",
       "      <th>agbd_a4</th>\n",
       "      <th>agbd_a5</th>\n",
       "      <th>agbd_a6</th>\n",
       "      <th>agbd_pi_lower</th>\n",
       "      <th>agbd_pi_lower_a1</th>\n",
       "      <th>...</th>\n",
       "      <th>xvar_a5_3</th>\n",
       "      <th>xvar_a5_4</th>\n",
       "      <th>xvar_a6_1</th>\n",
       "      <th>xvar_a6_2</th>\n",
       "      <th>xvar_a6_3</th>\n",
       "      <th>xvar_a6_4</th>\n",
       "      <th>xvar_a10_1</th>\n",
       "      <th>xvar_a10_2</th>\n",
       "      <th>xvar_a10_3</th>\n",
       "      <th>xvar_a10_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "      <td>233736.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-5723.515137</td>\n",
       "      <td>-5729.544922</td>\n",
       "      <td>-5699.986816</td>\n",
       "      <td>-5704.903320</td>\n",
       "      <td>-5724.968750</td>\n",
       "      <td>-5733.004395</td>\n",
       "      <td>-5694.802734</td>\n",
       "      <td>-5708.046875</td>\n",
       "      <td>-8903.421875</td>\n",
       "      <td>-8916.979492</td>\n",
       "      <td>...</td>\n",
       "      <td>-5717.515625</td>\n",
       "      <td>-5719.214355</td>\n",
       "      <td>-5717.739746</td>\n",
       "      <td>-5719.590820</td>\n",
       "      <td>-5720.391113</td>\n",
       "      <td>-5722.080566</td>\n",
       "      <td>-5714.857910</td>\n",
       "      <td>-5716.710449</td>\n",
       "      <td>-5717.522461</td>\n",
       "      <td>-5719.214355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4960.680664</td>\n",
       "      <td>4959.826172</td>\n",
       "      <td>4970.760254</td>\n",
       "      <td>4967.098145</td>\n",
       "      <td>4958.889648</td>\n",
       "      <td>4955.729980</td>\n",
       "      <td>4975.967773</td>\n",
       "      <td>4959.737793</td>\n",
       "      <td>3130.763428</td>\n",
       "      <td>3111.988525</td>\n",
       "      <td>...</td>\n",
       "      <td>4944.986328</td>\n",
       "      <td>4942.834961</td>\n",
       "      <td>4949.347656</td>\n",
       "      <td>4952.636719</td>\n",
       "      <td>4951.495605</td>\n",
       "      <td>4946.908203</td>\n",
       "      <td>4949.427246</td>\n",
       "      <td>4946.170898</td>\n",
       "      <td>4944.979004</td>\n",
       "      <td>4942.834961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.459848</td>\n",
       "      <td>1.453466</td>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.977234</td>\n",
       "      <td>1.214101</td>\n",
       "      <td>1.061925</td>\n",
       "      <td>0.980942</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.033444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.031450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4549.524902</td>\n",
       "      <td>4549.524902</td>\n",
       "      <td>4915.053711</td>\n",
       "      <td>4917.447754</td>\n",
       "      <td>4538.434570</td>\n",
       "      <td>2850.412842</td>\n",
       "      <td>5066.130371</td>\n",
       "      <td>4919.843262</td>\n",
       "      <td>3862.254395</td>\n",
       "      <td>3862.254395</td>\n",
       "      <td>...</td>\n",
       "      <td>13.695985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.383758</td>\n",
       "      <td>14.402083</td>\n",
       "      <td>12.912785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.381159</td>\n",
       "      <td>14.402083</td>\n",
       "      <td>13.406714</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                agbd        agbd_a1       agbd_a10        agbd_a2  \\\n",
       "count  233736.000000  233736.000000  233736.000000  233736.000000   \n",
       "mean    -5723.515137   -5729.544922   -5699.986816   -5704.903320   \n",
       "std      4960.680664    4959.826172    4970.760254    4967.098145   \n",
       "min     -9999.000000   -9999.000000   -9999.000000   -9999.000000   \n",
       "25%     -9999.000000   -9999.000000   -9999.000000   -9999.000000   \n",
       "50%     -9999.000000   -9999.000000   -9999.000000   -9999.000000   \n",
       "75%         1.459848       1.453466       0.970350       0.970350   \n",
       "max      4549.524902    4549.524902    4915.053711    4917.447754   \n",
       "\n",
       "             agbd_a3        agbd_a4        agbd_a5        agbd_a6  \\\n",
       "count  233736.000000  233736.000000  233736.000000  233736.000000   \n",
       "mean    -5724.968750   -5733.004395   -5694.802734   -5708.046875   \n",
       "std      4958.889648    4955.729980    4975.967773    4959.737793   \n",
       "min     -9999.000000   -9999.000000   -9999.000000   -9999.000000   \n",
       "25%     -9999.000000   -9999.000000   -9999.000000   -9999.000000   \n",
       "50%     -9999.000000   -9999.000000   -9999.000000   -9999.000000   \n",
       "75%         0.977234       1.214101       1.061925       0.980942   \n",
       "max      4538.434570    2850.412842    5066.130371    4919.843262   \n",
       "\n",
       "       agbd_pi_lower  agbd_pi_lower_a1  ...      xvar_a5_3      xvar_a5_4  \\\n",
       "count  233736.000000     233736.000000  ...  233736.000000  233736.000000   \n",
       "mean    -8903.421875      -8916.979492  ...   -5717.515625   -5719.214355   \n",
       "std      3130.763428       3111.988525  ...    4944.986328    4942.834961   \n",
       "min     -9999.000000      -9999.000000  ...   -9999.000000   -9999.000000   \n",
       "25%     -9999.000000      -9999.000000  ...   -9999.000000   -9999.000000   \n",
       "50%     -9999.000000      -9999.000000  ...   -9999.000000   -9999.000000   \n",
       "75%     -9999.000000      -9999.000000  ...       0.000000       0.000000   \n",
       "max      3862.254395       3862.254395  ...      13.695985       0.000000   \n",
       "\n",
       "           xvar_a6_1      xvar_a6_2      xvar_a6_3      xvar_a6_4  \\\n",
       "count  233736.000000  233736.000000  233736.000000  233736.000000   \n",
       "mean    -5717.739746   -5719.590820   -5720.391113   -5722.080566   \n",
       "std      4949.347656    4952.636719    4951.495605    4946.908203   \n",
       "min     -9999.000000   -9999.000000   -9999.000000   -9999.000000   \n",
       "25%     -9999.000000   -9999.000000   -9999.000000   -9999.000000   \n",
       "50%     -9999.000000   -9999.000000   -9999.000000   -9999.000000   \n",
       "75%        10.033444       0.000000       0.000000       0.000000   \n",
       "max        15.383758      14.402083      12.912785       0.000000   \n",
       "\n",
       "          xvar_a10_1     xvar_a10_2     xvar_a10_3     xvar_a10_4  \n",
       "count  233736.000000  233736.000000  233736.000000  233736.000000  \n",
       "mean    -5714.857910   -5716.710449   -5717.522461   -5719.214355  \n",
       "std      4949.427246    4946.170898    4944.979004    4942.834961  \n",
       "min     -9999.000000   -9999.000000   -9999.000000   -9999.000000  \n",
       "25%     -9999.000000   -9999.000000   -9999.000000   -9999.000000  \n",
       "50%     -9999.000000   -9999.000000   -9999.000000   -9999.000000  \n",
       "75%        10.031450       0.000000       0.000000       0.000000  \n",
       "max        15.381159      14.402083      13.406714       0.000000  \n",
       "\n",
       "[8 rows x 201 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally, save the complete DataFrame for further processing:\n",
    "# Get a list of column names\n",
    "columns = complete_df.columns\n",
    "\n",
    "# Find duplicates\n",
    "duplicate_columns = columns[columns.duplicated()]\n",
    "print(\"Duplicate columns:\", duplicate_columns)\n",
    "print('There are ' + str(columns.duplicated().sum()) + ' duplicated columns.')\n",
    "print('NaN value count:\\n')\n",
    "print(complete_df.isnull().sum())\n",
    "complete_df.describe()\n",
    "\n",
    "# complete_df.to_parquet(\"L4A_and_ALS.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "# print('Successfully wrote to a .parquet file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73cf29",
   "metadata": {},
   "source": [
    "#### Feature engineering\n",
    "1. Adding Z-Scores for five most relevant RH metrics\n",
    "2. Creating RH50 / RH100 ratio\n",
    "3. Adding RH95 - RH50\n",
    "4. Adding \"Missingness\" - the number of NaNs in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zeros = (complete_df['RH_100'] == 0).sum()\n",
    "print(f\"Number of zeros in RH_100: {num_zeros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ed4109accefc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T15:26:48.589282Z",
     "start_time": "2024-10-28T15:26:48.291657Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Add Z-Scores of the five RH metrics'''\n",
    "rh_nums = [25, 50, 75, 85, 95, 100]\n",
    "for i in rh_nums:\n",
    "    col_name = f'RH_{i}'\n",
    "    complete_df[f'{col_name} Z Score'] = (complete_df[col_name] - complete_df[col_name].mean()) / complete_df[col_name].std()\n",
    "\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a756676",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding the RH_50 / RH_100 feature'''\n",
    "complete_df['RH_50_v_100'] = complete_df['RH_50'] / complete_df['RH_100']\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding the (RH95 - RH50) feature'''\n",
    "rh_50 = complete_df['RH_50']\n",
    "rh_95 = complete_df['RH_95']\n",
    "complete_df['RH_95_minus_50'] = (rh_95 - rh_50)\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding the 'Missingness' feature'''\n",
    "# Count the number of NaNs in each row\n",
    "complete_df['Missingness'] = complete_df.isna().sum(axis=1)\n",
    "\n",
    "\n",
    "# Optionally, inspect how many rows have missing data\n",
    "print('Number of NaNs:')\n",
    "print(complete_df['Missingness'].value_counts())\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1187d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Optional: Save to a parquet file'''\n",
    "# Get a list of column names\n",
    "columns = complete_df.columns\n",
    "\n",
    "# Find duplicates\n",
    "duplicate_columns = columns[columns.duplicated()]\n",
    "print(\"Duplicate columns:\", duplicate_columns)\n",
    "print('There are ' + str(columns.duplicated().sum()) + ' duplicated columns.')\n",
    "print('NaN value count:\\n')\n",
    "print(complete_df.isnull().sum())\n",
    "# complete_df.describe()\n",
    "\n",
    "# Uncomment the two lines below to write to parquet\n",
    "# complete_df.to_parquet(\"input_raw.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "# print('Successfully wrote to a .parquet file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307ec82",
   "metadata": {},
   "source": [
    "#### This is the filtering step for PCA if there are any NaN rows in the dataframe\n",
    "Note: there should not be mnany NaN values if you accounted for the different shapes of the data inside the HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Optional: load complete_df from .parquet file'''\n",
    "complete_df = pd.read_parquet('input_raw.parquet', engine='pyarrow')\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e344202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(f'Original dataframe shape: {complete_df.shape}')\n",
    "\n",
    "'''Filtering'''\n",
    "complete_df.drop('Shot Number', axis=1, inplace=True)\n",
    "print(\"Shot numbers dropped\")\n",
    "discounted_df = complete_df.dropna(axis=1)\n",
    "\n",
    "\n",
    "# imputer = SimpleImputer(strategy='most_frequent')  # You can change to 'median', 'most_frequent', etc.\n",
    "# discounted_df = pd.DataFrame(imputer.fit_transform(complete_df), columns=complete_df.columns)\n",
    "\n",
    "print(\"Dataframe shape after dropping NaN columns:\", discounted_df.shape)\n",
    "\n",
    "# Step 1: Separate features and target (if applicable)\n",
    "# Exclude non-numeric columns if present\n",
    "filtered_df = discounted_df.select_dtypes(include=[np.number])\n",
    "# columns_to_keep = [col for col in numeric_df.columns if 'RH' not in col]\n",
    "# filtered_df = filtered_df[columns_to_keep]\n",
    "\n",
    "print(f'Non-numeric columns removed from dataframe\\nCleaned dataframe size: {filtered_df.shape}')\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97056d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2, OPTION 1: Standardize the data using StandardScalar (for demo only)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaled_data = scaler.fit_transform(filtered_df)\n",
    "# print(f'Data scaled\\nScaled data size (ndarray): {scaled_data.shape}')\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=filtered_df.columns, index=filtered_df.index)\n",
    "print(\"Made new scaled dataframe\")\n",
    "scaled_df\n",
    "scaled_df.describe()\n",
    "\n",
    "# Uncomment the two lines below to write scaled_df to parquet\n",
    "scaled_df.to_parquet(\"input_standard_scaled.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "print('Successfully wrote to a .parquet file!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a602a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: Standardize the data using RobustScalar (this is better for outlier detection)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Assume `filtered_df` is your cleaned and filtered dataframe (all numeric columns)\n",
    "robust_scaler = RobustScaler()\n",
    "print(\"Instantiated RobustScalar\")\n",
    "\n",
    "# Fit the scaler on the dataframe and transform it\n",
    "scaled_array = robust_scaler.fit_transform(filtered_df)\n",
    "print(\"Fitted RobustScalar\")\n",
    "\n",
    "# (Optional) Create a new DataFrame with scaled values\n",
    "scaled_df = pd.DataFrame(scaled_array, columns=filtered_df.columns, index=filtered_df.index)\n",
    "print(\"Made new scaled dataframe\")\n",
    "scaled_df\n",
    "scaled_df.describe()\n",
    "\n",
    "# Uncomment the two lines below to write scaled_df to parquet\n",
    "scaled_df.to_parquet(\"input_scaled.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "print('Successfully wrote to a .parquet file!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEDI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
