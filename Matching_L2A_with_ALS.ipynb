{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43779e86e13e3b84",
   "metadata": {},
   "source": [
    "# Matching shot_number between GEDI L2A & ALS\n",
    "We will match shot numbers in the Level 2A GEDI data with the ALS crossover data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:12.404923Z",
     "start_time": "2024-12-03T16:42:12.088600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "# import geoviews as gv\n",
    "# from geoviews import opts, tile_sources as gvts\n",
    "# import holoviews as hv\n",
    "# gv.extension('bokeh', 'matplotlib')\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# import PyQt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abfb1225ff906d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:12.916873Z",
     "start_time": "2024-12-03T16:42:12.903957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/oscar/scratch/jzhu118/GEDI_Outlier_Detection_OSCAR/Input_files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['GEDI02_A_2021050140102_O12405_02_T10912_02_003_02_V002.h5',\n",
       " 'GEDI02_A_2021086153349_O12964_03_T08275_02_003_02_V002.h5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inDir = os.getcwd() + \"/Input_files\"\n",
    "print(inDir)\n",
    "input_file_names = [g for g in os.listdir(inDir) if g.startswith('GEDI02_A') and g.endswith('.h5')]  # List all GEDI level 2 files in inDir\n",
    "input_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193cf3a589257efc",
   "metadata": {},
   "source": [
    "### Loading files with all information into a huge Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ca866",
   "metadata": {},
   "source": [
    "#### Preprocessing: Get files and sort by beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a37290db29772d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:13.026872Z",
     "start_time": "2024-12-03T16:42:12.947179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: GEDI02_A_2021050140102_O12405_02_T10912_02_003_02_V002.h5\n",
      "The file contains the following groups: ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011', 'METADATA']\n",
      "The file's metadata contains the following attributes: \n",
      "PGEVersion\n",
      "VersionID\n",
      "abstract\n",
      "characterSet\n",
      "creationDate\n",
      "credit\n",
      "fileName\n",
      "language\n",
      "originatorOrganizationName\n",
      "purpose\n",
      "shortName\n",
      "spatialRepresentationType\n",
      "status\n",
      "topicCategory\n",
      "uuid\n",
      "The file contains the following beams: \n",
      "BEAM0000 is a Coverage beam\n",
      "BEAM0001 is a Coverage beam\n",
      "BEAM0010 is a Coverage beam\n",
      "BEAM0011 is a Coverage beam\n",
      "BEAM0101 is a Full power beam\n",
      "BEAM0110 is a Full power beam\n",
      "BEAM1000 is a Full power beam\n",
      "BEAM1011 is a Full power beam\n",
      "Loading file: GEDI02_A_2021086153349_O12964_03_T08275_02_003_02_V002.h5\n",
      "The file contains the following groups: ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011', 'METADATA']\n",
      "The file's metadata contains the following attributes: \n",
      "PGEVersion\n",
      "VersionID\n",
      "abstract\n",
      "characterSet\n",
      "creationDate\n",
      "credit\n",
      "fileName\n",
      "language\n",
      "originatorOrganizationName\n",
      "purpose\n",
      "shortName\n",
      "spatialRepresentationType\n",
      "status\n",
      "topicCategory\n",
      "uuid\n",
      "The file contains the following beams: \n",
      "BEAM0000 is a Coverage beam\n",
      "BEAM0001 is a Coverage beam\n",
      "BEAM0010 is a Coverage beam\n",
      "BEAM0011 is a Coverage beam\n",
      "BEAM0101 is a Full power beam\n",
      "BEAM0110 is a Full power beam\n",
      "BEAM1000 is a Full power beam\n",
      "BEAM1011 is a Full power beam\n"
     ]
    }
   ],
   "source": [
    "input_files = []\n",
    "files_to_beams = dict()\n",
    "for n in input_file_names:\n",
    "    file_path = os.path.join(inDir, n)  # Select an example file\n",
    "    file = h5py.File(file_path, 'r')\n",
    "    input_files.append(file)\n",
    "    \n",
    "    print('Loading file: ' + n)\n",
    "    print('The file contains the following groups: ' + str(list(file.keys())))\n",
    "    \n",
    "    print(\"The file's metadata contains the following attributes: \")\n",
    "    for g in file['METADATA']['DatasetIdentification'].attrs: print(g)\n",
    "    \n",
    "    beamNames = [g for g in file.keys() if g.startswith('BEAM')]\n",
    "    files_to_beams[file] = beamNames\n",
    "    \n",
    "    print(\"The file contains the following beams: \")\n",
    "    for b in beamNames:\n",
    "        print(f\"{b} is a {file[b].attrs['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14096b",
   "metadata": {},
   "source": [
    "#### Helper functions for the main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660146583f485f77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:14.006557Z",
     "start_time": "2024-12-03T16:42:13.992165Z"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def collect_all_datasets(file_path):\n",
    "    \"\"\"\n",
    "    Recursively collects all dataset paths within the HDF5 file.\n",
    "    Returns a list of dataset path strings.\n",
    "    \"\"\"\n",
    "    dataset_paths = []\n",
    "    \n",
    "    def visitor_func(name, node):\n",
    "        if isinstance(node, h5py.Dataset):\n",
    "            dataset_paths.append(name)\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        f.visititems(visitor_func)\n",
    "    \n",
    "    return dataset_paths\n",
    "\n",
    "def get_dataset_by_name(h5_file, beam_ds, name):\n",
    "    \"\"\"\n",
    "    Given an open HDF5 file (h5_file) and a list of dataset paths (beam_ds),\n",
    "    returns the data (as a NumPy array) for the first dataset whose path ends\n",
    "    with '/{name}'. If not found, returns None.\n",
    "    \"\"\"\n",
    "    candidates = [g for g in beam_ds if g.endswith(f'/{name}')]\n",
    "    if not candidates:\n",
    "        print(f\"Warning: No dataset ending with '/{name}' found.\")\n",
    "        return None\n",
    "    dataset_path = candidates[0]\n",
    "    return h5_file[dataset_path][()]  # This reads the dataset into memory\n",
    "\n",
    "def extract_rh_indices(rh_data, indices=[25, 50, 75, 85, 95, 100]):\n",
    "    \"\"\"\n",
    "    Given an array (rh_data) with shape (N, X) from the 'rh' dataset,\n",
    "    returns a dictionary mapping keys (e.g., 'RH_25') to arrays of values.\n",
    "    If rh_data is None, returns an empty dictionary.\n",
    "    \"\"\"\n",
    "    if rh_data is None:\n",
    "        return {}\n",
    "    \n",
    "    rh_array = np.array(rh_data)  # Ensure we have a NumPy array\n",
    "    result = {}\n",
    "    for i in indices:\n",
    "        col_name = f\"RH_{i}\"\n",
    "        try:\n",
    "            result[col_name] = rh_array[:, i]\n",
    "        except IndexError:\n",
    "            print(f\"Warning: Index {i} is out of bounds for the rh dataset.\")\n",
    "            result[col_name] = np.full(rh_array.shape[0], np.nan)\n",
    "    return result\n",
    "\n",
    "def load_attributes_to_dict(txt_file_path):\n",
    "    \"\"\"\n",
    "    Reads attribute names from a text file (one per line) and returns a dictionary.\n",
    "    Each attribute is mapped to itself.\n",
    "    For example:\n",
    "      { \"Channel\": \"Channel\", \"digital_elevation_model\": \"digital_elevation_model\", ... }\n",
    "    \"\"\"\n",
    "    with open(txt_file_path, 'r') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    attr_dict = {}\n",
    "    for raw_attr in lines:\n",
    "        attr_dict[raw_attr] = raw_attr  # Direct mapping for simplicity\n",
    "    return attr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a951e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ALS crossover data with 76778 unique shot numbers.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# LOAD ALS CROSSOVER DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "als_csv_path = os.path.join('Input_files', 'GEDI_ALSCROSSOVERS.csv')\n",
    "als_df = pd.read_csv(als_csv_path, dtype={'shot_number': str})\n",
    "\n",
    "# Ensure shot numbers are of a consistent type (adjust if necessary)\n",
    "als_df['shot_number'] = als_df['shot_number'].astype(str)\n",
    "als_shot_set = set(als_df['shot_number'])\n",
    "print(f\"Loaded ALS crossover data with {len(als_shot_set)} unique shot numbers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b7aa9",
   "metadata": {},
   "source": [
    "### THIS CELL IS FOR TESTING ONLY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e948bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 216338 shot numbers for als_shot_set.\n"
     ]
    }
   ],
   "source": [
    "# Read the parquet file containing all GEDI waveform shots\n",
    "gedi_df = pd.read_parquet('L2A_raw.parquet')\n",
    "\n",
    "# Assume the shot numbers are in a column named 'shot_number'\n",
    "# Randomly sample 10% of the shot numbers for testing purposes.\n",
    "# Set a random state for reproducibility.\n",
    "sample_fraction = 0.1\n",
    "random_state = 42\n",
    "\n",
    "als_shot_series = gedi_df['Shot Number'].sample(frac=sample_fraction, random_state=random_state)\n",
    "\n",
    "# Convert shot numbers to strings for consistent matching later on\n",
    "als_shot_set = set(als_shot_series.astype(str))\n",
    "\n",
    "print(f\"Selected {len(als_shot_set)} shot numbers for als_shot_set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d7a3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded attributes: {'back_threshold': 'back_threshold', 'beam': 'beam', 'botloc': 'botloc', 'botloc_amp': 'botloc_amp', 'channel': 'channel', 'degrade_flag': 'degrade_flag', 'delta_time': 'delta_time', 'digital_elevation_model': 'digital_elevation_model', 'digital_elevation_model_srtm': 'digital_elevation_model_srtm', 'elev_highestreturn': 'elev_highestreturn', 'elev_highestreturn_a1': 'elev_highestreturn_a1', 'elev_highestreturn_a2': 'elev_highestreturn_a2', 'elev_highestreturn_a3': 'elev_highestreturn_a3', 'elev_highestreturn_a4': 'elev_highestreturn_a4', 'elev_highestreturn_a5': 'elev_highestreturn_a5', 'elev_highestreturn_a6': 'elev_highestreturn_a6', 'elev_lowestmode': 'elev_lowestmode', 'elev_lowestmode_a1': 'elev_lowestmode_a1', 'elev_lowestmode_a2': 'elev_lowestmode_a2', 'elev_lowestmode_a3': 'elev_lowestmode_a3', 'elev_lowestmode_a4': 'elev_lowestmode_a4', 'elev_lowestmode_a5': 'elev_lowestmode_a5', 'elev_lowestmode_a6': 'elev_lowestmode_a6', 'elev_lowestreturn_a1': 'elev_lowestreturn_a1', 'elev_lowestreturn_a2': 'elev_lowestreturn_a2', 'elev_lowestreturn_a3': 'elev_lowestreturn_a3', 'elev_lowestreturn_a4': 'elev_lowestreturn_a4', 'elev_lowestreturn_a5': 'elev_lowestreturn_a5', 'elev_lowestreturn_a6': 'elev_lowestreturn_a6', 'elevation_1gfit': 'elevation_1gfit', 'elevation_bias_flag': 'elevation_bias_flag', 'elevation_bin0_error': 'elevation_bin0_error', 'energy_lowestmode_a1': 'energy_lowestmode_a1', 'energy_lowestmode_a2': 'energy_lowestmode_a2', 'energy_lowestmode_a3': 'energy_lowestmode_a3', 'energy_lowestmode_a4': 'energy_lowestmode_a4', 'energy_lowestmode_a5': 'energy_lowestmode_a5', 'energy_lowestmode_a6': 'energy_lowestmode_a6', 'energy_sm': 'energy_sm', 'energy_total': 'energy_total', 'front_threshold': 'front_threshold', 'landsat_treecover': 'landsat_treecover', 'landsat_water_persistence': 'landsat_water_persistence', 'lastmodeenergy': 'lastmodeenergy', 'lat_highestreturn': 'lat_highestreturn', 'lat_highestreturn_a1': 'lat_highestreturn_a1', 'lat_highestreturn_a2': 'lat_highestreturn_a2', 'lat_highestreturn_a3': 'lat_highestreturn_a3', 'lat_highestreturn_a4': 'lat_highestreturn_a4', 'lat_highestreturn_a5': 'lat_highestreturn_a5', 'lat_highestreturn_a6': 'lat_highestreturn_a6', 'lat_lowestmode': 'lat_lowestmode', 'lat_lowestmode_a1': 'lat_lowestmode_a1', 'lat_lowestmode_a2': 'lat_lowestmode_a2', 'lat_lowestmode_a3': 'lat_lowestmode_a3', 'lat_lowestmode_a4': 'lat_lowestmode_a4', 'lat_lowestmode_a5': 'lat_lowestmode_a5', 'lat_lowestmode_a6': 'lat_lowestmode_a6', 'lat_lowestreturn_a1': 'lat_lowestreturn_a1', 'lat_lowestreturn_a2': 'lat_lowestreturn_a2', 'lat_lowestreturn_a3': 'lat_lowestreturn_a3', 'lat_lowestreturn_a4': 'lat_lowestreturn_a4', 'lat_lowestreturn_a5': 'lat_lowestreturn_a5', 'lat_lowestreturn_a6': 'lat_lowestreturn_a6', 'latitude_1gfit': 'latitude_1gfit', 'latitude_bin0_error': 'latitude_bin0_error', 'leaf_off_doy': 'leaf_off_doy', 'leaf_off_flag': 'leaf_off_flag', 'leaf_on_cycle': 'leaf_on_cycle', 'leaf_on_doy': 'leaf_on_doy', 'lon_highestreturn': 'lon_highestreturn', 'lon_highestreturn_a1': 'lon_highestreturn_a1', 'lon_highestreturn_a2': 'lon_highestreturn_a2', 'lon_highestreturn_a3': 'lon_highestreturn_a3', 'lon_highestreturn_a4': 'lon_highestreturn_a4', 'lon_highestreturn_a5': 'lon_highestreturn_a5', 'lon_highestreturn_a6': 'lon_highestreturn_a6', 'lon_lowestmode': 'lon_lowestmode', 'lon_lowestmode_a1': 'lon_lowestmode_a1', 'lon_lowestmode_a2': 'lon_lowestmode_a2', 'lon_lowestmode_a3': 'lon_lowestmode_a3', 'lon_lowestmode_a4': 'lon_lowestmode_a4', 'lon_lowestmode_a5': 'lon_lowestmode_a5', 'lon_lowestmode_a6': 'lon_lowestmode_a6', 'lon_lowestreturn_a1': 'lon_lowestreturn_a1', 'lon_lowestreturn_a2': 'lon_lowestreturn_a2', 'lon_lowestreturn_a3': 'lon_lowestreturn_a3', 'lon_lowestreturn_a4': 'lon_lowestreturn_a4', 'lon_lowestreturn_a5': 'lon_lowestreturn_a5', 'lon_lowestreturn_a6': 'lon_lowestreturn_a6', 'longitude_1gfit': 'longitude_1gfit', 'longitude_bin0_error': 'longitude_bin0_error', 'master_frac': 'master_frac', 'master_int': 'master_int', 'mean': 'mean', 'mean_64kadjusted': 'mean_64kadjusted', 'mean_sea_surface': 'mean_sea_surface', 'mean_sm': 'mean_sm', 'min_detection_energy': 'min_detection_energy', 'min_detection_threshold': 'min_detection_threshold', 'modis_nonvegetated': 'modis_nonvegetated', 'modis_nonvegetated_sd': 'modis_nonvegetated_sd', 'modis_treecover': 'modis_treecover', 'modis_treecover_sd': 'modis_treecover_sd', 'num_detectedmodes': 'num_detectedmodes', 'num_detectedmodes_a1': 'num_detectedmodes_a1', 'num_detectedmodes_a2': 'num_detectedmodes_a2', 'num_detectedmodes_a3': 'num_detectedmodes_a3', 'num_detectedmodes_a4': 'num_detectedmodes_a4', 'num_detectedmodes_a5': 'num_detectedmodes_a5', 'num_detectedmodes_a6': 'num_detectedmodes_a6', 'ocean_calibration_shot_flag': 'ocean_calibration_shot_flag', 'peak': 'peak', 'pft_class': 'pft_class', 'pk_sm': 'pk_sm', 'quality_flag': 'quality_flag', 'quality_flag_a1': 'quality_flag_a1', 'quality_flag_a2': 'quality_flag_a2', 'quality_flag_a3': 'quality_flag_a3', 'quality_flag_a4': 'quality_flag_a4', 'quality_flag_a5': 'quality_flag_a5', 'quality_flag_a6': 'quality_flag_a6', 'region_class': 'region_class', 'rx_algrunflag': 'rx_algrunflag', 'rx_assess_flag': 'rx_assess_flag', 'rx_clipbin0': 'rx_clipbin0', 'rx_clipbin_count': 'rx_clipbin_count', 'rx_energy': 'rx_energy', 'rx_gamplitude': 'rx_gamplitude', 'rx_gamplitude_error': 'rx_gamplitude_error', 'rx_gbias': 'rx_gbias', 'rx_gbias_error': 'rx_gbias_error', 'rx_gchisq': 'rx_gchisq', 'rx_gflag': 'rx_gflag', 'rx_giters': 'rx_giters', 'rx_gloc': 'rx_gloc', 'rx_gloc_error': 'rx_gloc_error', 'rx_gwidth': 'rx_gwidth', 'rx_gwidth_error': 'rx_gwidth_error', 'rx_maxamp': 'rx_maxamp', 'rx_maxpeakloc': 'rx_maxpeakloc', 'rx_nummodes': 'rx_nummodes', 'sd_corrected': 'sd_corrected', 'sd_sm': 'sd_sm', 'search_end': 'search_end', 'search_start': 'search_start', 'selected_algorithm': 'selected_algorithm', 'selected_mode': 'selected_mode', 'selected_mode_flag': 'selected_mode_flag', 'sensitivity': 'sensitivity', 'sensitivity_a1': 'sensitivity_a1', 'sensitivity_a2': 'sensitivity_a2', 'sensitivity_a3': 'sensitivity_a3', 'sensitivity_a4': 'sensitivity_a4', 'sensitivity_a5': 'sensitivity_a5', 'sensitivity_a6': 'sensitivity_a6', 'shot_number': 'shot_number', 'smoothwidth': 'smoothwidth', 'smoothwidth_zcross': 'smoothwidth_zcross', 'solar_azimuth': 'solar_azimuth', 'solar_elevation': 'solar_elevation', 'stale_return_flag': 'stale_return_flag', 'stddev': 'stddev', 'surface_flag': 'surface_flag', 'toploc': 'toploc', 'toploc_miss': 'toploc_miss', 'urban_focal_window_size': 'urban_focal_window_size', 'urban_proportion': 'urban_proportion', 'zcross': 'zcross', 'zcross0': 'zcross0', 'zcross_amp': 'zcross_amp', 'zcross_localenergy': 'zcross_localenergy'}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# LOAD ATTRIBUTE DICTIONARY (including Beam and Channel, among others)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "txt_path = 'L2A_features.txt'\n",
    "ATTRIBUTES_TO_LOAD = load_attributes_to_dict(txt_path)\n",
    "print(\"Loaded attributes:\", ATTRIBUTES_TO_LOAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4574d5",
   "metadata": {},
   "source": [
    "#### Main loop, collects all target features and puts them inside a Pandas dataframe.\n",
    "WARNING: Despite my best attempts at optimization, this is *extremely* memory intensive even for one 5GB HDF5 file. Only run on a supercomputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe575445c09a1d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:50:27.138768Z",
     "start_time": "2024-12-03T17:37:18.265405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: /oscar/scratch/jzhu118/GEDI_Outlier_Detection_OSCAR/Input_files/GEDI02_A_2021050140102_O12405_02_T10912_02_003_02_V002.h5\n",
      "  Processing beam: BEAM0000\n",
      "    Found 10220 matching shot(s).\n",
      "    Processed beam BEAM0000 with 10220 matching shots.\n",
      "  Processing beam: BEAM0001\n",
      "    Found 10103 matching shot(s).\n",
      "    Processed beam BEAM0001 with 10103 matching shots.\n",
      "  Processing beam: BEAM0010\n",
      "    Found 10388 matching shot(s).\n",
      "    Processed beam BEAM0010 with 10388 matching shots.\n",
      "  Processing beam: BEAM0011\n",
      "    Found 10142 matching shot(s).\n",
      "    Processed beam BEAM0011 with 10142 matching shots.\n",
      "  Processing beam: BEAM0101\n",
      "    Found 10134 matching shot(s).\n",
      "    Processed beam BEAM0101 with 10134 matching shots.\n",
      "  Processing beam: BEAM0110\n",
      "    Found 11134 matching shot(s).\n",
      "    Processed beam BEAM0110 with 11134 matching shots.\n",
      "  Processing beam: BEAM1000\n",
      "    Found 10320 matching shot(s).\n",
      "    Processed beam BEAM1000 with 10320 matching shots.\n",
      "  Processing beam: BEAM1011\n",
      "    Found 10080 matching shot(s).\n",
      "    Processed beam BEAM1011 with 10080 matching shots.\n",
      "\n",
      "Processing file: /oscar/scratch/jzhu118/GEDI_Outlier_Detection_OSCAR/Input_files/GEDI02_A_2021086153349_O12964_03_T08275_02_003_02_V002.h5\n",
      "  Processing beam: BEAM0000\n",
      "    Found 16793 matching shot(s).\n",
      "    Processed beam BEAM0000 with 16793 matching shots.\n",
      "  Processing beam: BEAM0001\n",
      "    Found 16586 matching shot(s).\n",
      "    Processed beam BEAM0001 with 16586 matching shots.\n",
      "  Processing beam: BEAM0010\n",
      "    Found 16737 matching shot(s).\n",
      "    Processed beam BEAM0010 with 16737 matching shots.\n",
      "  Processing beam: BEAM0011\n",
      "    Found 16898 matching shot(s).\n",
      "    Processed beam BEAM0011 with 16898 matching shots.\n",
      "  Processing beam: BEAM0101\n",
      "    Found 16945 matching shot(s).\n",
      "    Processed beam BEAM0101 with 16945 matching shots.\n",
      "  Processing beam: BEAM0110\n",
      "    Found 16533 matching shot(s).\n",
      "    Processed beam BEAM0110 with 16533 matching shots.\n",
      "  Processing beam: BEAM1000\n",
      "    Found 16473 matching shot(s).\n",
      "    Processed beam BEAM1000 with 16473 matching shots.\n",
      "  Processing beam: BEAM1011\n",
      "    Found 16852 matching shot(s).\n",
      "    Processed beam BEAM1011 with 16852 matching shots.\n",
      "ALL DONE!\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# PROCESS GEDI LEVEL 2A FILES & MATCH SHOT NUMBERS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for f in input_files:\n",
    "    print(f\"\\nProcessing file: {f.filename}\")\n",
    "    \n",
    "    # Collect all dataset paths in the current file\n",
    "    all_ds_for_file = collect_all_datasets(f.filename)\n",
    "    \n",
    "    # Process each beam in the current file\n",
    "    for b in files_to_beams[f]:\n",
    "        print(f\"  Processing beam: {b}\")\n",
    "        \n",
    "        # Filter dataset paths to only those belonging to the current beam\n",
    "        beam_ds = [ds for ds in all_ds_for_file if b in ds]\n",
    "        \n",
    "        # 1) Retrieve shot numbers for the beam and convert to string for matching\n",
    "        shotNums = get_dataset_by_name(f, beam_ds, 'shot_number')\n",
    "        if shotNums is None:\n",
    "            print(f\"    Warning: No 'shot_number' dataset found for beam {b}. Skipping beam.\")\n",
    "            continue\n",
    "        shotNums = np.array(shotNums).astype(str)\n",
    "        \n",
    "        # 2) Identify indices where GEDI shot numbers match the ALS crossover shot numbers\n",
    "        matching_indices = np.where(np.isin(shotNums, list(als_shot_set)))[0]\n",
    "        if matching_indices.size == 0:\n",
    "            print(f\"    No matching shot numbers found for beam {b}.\")\n",
    "            continue\n",
    "        row_count = matching_indices.size\n",
    "        print(f\"    Found {row_count} matching shot(s).\")\n",
    "        \n",
    "        # Build constant columns: File Name and Beam Name\n",
    "        file_name_col = [os.path.basename(f.filename)] * row_count\n",
    "        beam_name_col = [b] * row_count\n",
    "        \n",
    "        # 3) Extract all other attributes as specified in the txt file\n",
    "        attr_data = {}\n",
    "        for col_label, ds_suffix in ATTRIBUTES_TO_LOAD.items():\n",
    "            data_array = get_dataset_by_name(f, beam_ds, ds_suffix)\n",
    "            if data_array is None:\n",
    "                print(f\"    Warning: Missing dataset for {ds_suffix}. Filling with None.\")\n",
    "                attr_data[col_label] = np.array([None] * len(shotNums))[matching_indices]\n",
    "            else:\n",
    "                data_array = np.array(data_array)\n",
    "                if data_array.shape[0] != shotNums.shape[0]:\n",
    "                    print(f\"    Warning: Row count mismatch for {col_label} in beam {b}.\")\n",
    "                attr_data[col_label] = data_array[matching_indices]\n",
    "        \n",
    "        # 4) Extract RH indices and filter by matching indices\n",
    "        rh_data = get_dataset_by_name(f, beam_ds, 'rh')\n",
    "        rh_dict_full = extract_rh_indices(rh_data)\n",
    "        rh_dict = {}\n",
    "        for key, arr in rh_dict_full.items():\n",
    "            arr = np.array(arr)\n",
    "            if arr.shape[0] != shotNums.shape[0]:\n",
    "                print(f\"    Warning: Row count mismatch for {key} in beam {b}.\")\n",
    "            rh_dict[key] = arr[matching_indices]\n",
    "        \n",
    "        # 5) Filter the shot numbers to only the matching ones\n",
    "        shotNums_filtered = shotNums[matching_indices]\n",
    "        \n",
    "        # 6) Construct the DataFrame for the current beam\n",
    "        df = pd.DataFrame({\n",
    "            'File Name': file_name_col,\n",
    "            'Beam Name': beam_name_col,\n",
    "            'Shot Number': shotNums_filtered,\n",
    "            **rh_dict,\n",
    "            **attr_data\n",
    "        })\n",
    "        \n",
    "        dataframes.append(df)\n",
    "        print(f\"    Processed beam {b} with {row_count} matching shots.\")\n",
    "\n",
    "print('ALL DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbdb9ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded beam name\n",
      "Encoded channel\n",
      "\n",
      "Final DataFrame shape: (216338, 193)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# FINALIZE: Concatenate all partial DataFrames into one final DataFrame\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if dataframes:\n",
    "    complete_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    if 'beam' in complete_df.columns:\n",
    "        complete_df = pd.get_dummies(complete_df, columns=['beam'], prefix='beam')\n",
    "        print('Encoded beam name')\n",
    "    if 'channel' in complete_df.columns:\n",
    "        complete_df = pd.get_dummies(complete_df, columns=['channel'], prefix='channel')\n",
    "        print('Encoded channel')\n",
    "\n",
    "    print(f\"\\nFinal DataFrame shape: {complete_df.shape}\")\n",
    "else:\n",
    "    complete_df = pd.DataFrame()\n",
    "    print(\"\\nNo matching data found in any beams.\")\n",
    "    \n",
    "# Optionally, save the complete DataFrame for further processing:\n",
    "# complete_df.to_parquet(\"ALS_and_L2A.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "# print('Successfully wrote to a .parquet file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73cf29",
   "metadata": {},
   "source": [
    "#### Feature engineering\n",
    "1. Adding Z-Scores for five most relevant RH metrics\n",
    "2. Creating RH50 / RH100 ratio\n",
    "3. Adding RH95 - RH50\n",
    "4. Adding \"Missingness\" - the number of NaNs in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zeros = (complete_df['RH_100'] == 0).sum()\n",
    "print(f\"Number of zeros in RH_100: {num_zeros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ed4109accefc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T15:26:48.589282Z",
     "start_time": "2024-10-28T15:26:48.291657Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Add Z-Scores of the five RH metrics'''\n",
    "rh_nums = [25, 50, 75, 85, 95, 100]\n",
    "for i in rh_nums:\n",
    "    col_name = f'RH_{i}'\n",
    "    complete_df[f'{col_name} Z Score'] = (complete_df[col_name] - complete_df[col_name].mean()) / complete_df[col_name].std()\n",
    "\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a756676",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding the RH_50 / RH_100 feature'''\n",
    "complete_df['RH_50_v_100'] = complete_df['RH_50'] / complete_df['RH_100']\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding the (RH95 - RH50) feature'''\n",
    "rh_50 = complete_df['RH_50']\n",
    "rh_95 = complete_df['RH_95']\n",
    "complete_df['RH_95_minus_50'] = (rh_95 - rh_50)\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding the 'Missingness' feature'''\n",
    "# Count the number of NaNs in each row\n",
    "complete_df['Missingness'] = complete_df.isna().sum(axis=1)\n",
    "\n",
    "\n",
    "# Optionally, inspect how many rows have missing data\n",
    "print('Number of NaNs:')\n",
    "print(complete_df['Missingness'].value_counts())\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1187d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Optional: Save to a parquet file'''\n",
    "# Get a list of column names\n",
    "columns = complete_df.columns\n",
    "\n",
    "# Find duplicates\n",
    "duplicate_columns = columns[columns.duplicated()]\n",
    "print(\"Duplicate columns:\", duplicate_columns)\n",
    "print('There are ' + str(columns.duplicated().sum()) + ' duplicated columns.')\n",
    "print('NaN value count:\\n')\n",
    "print(complete_df.isnull().sum())\n",
    "# complete_df.describe()\n",
    "\n",
    "# Uncomment the two lines below to write to parquet\n",
    "# complete_df.to_parquet(\"input_raw.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "# print('Successfully wrote to a .parquet file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307ec82",
   "metadata": {},
   "source": [
    "#### This is the filtering step for PCA if there are any NaN rows in the dataframe\n",
    "Note: there should not be mnany NaN values if you accounted for the different shapes of the data inside the HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Optional: load complete_df from .parquet file'''\n",
    "complete_df = pd.read_parquet('input_raw.parquet', engine='pyarrow')\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e344202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(f'Original dataframe shape: {complete_df.shape}')\n",
    "\n",
    "'''Filtering'''\n",
    "complete_df.drop('Shot Number', axis=1, inplace=True)\n",
    "print(\"Shot numbers dropped\")\n",
    "discounted_df = complete_df.dropna(axis=1)\n",
    "\n",
    "\n",
    "# imputer = SimpleImputer(strategy='most_frequent')  # You can change to 'median', 'most_frequent', etc.\n",
    "# discounted_df = pd.DataFrame(imputer.fit_transform(complete_df), columns=complete_df.columns)\n",
    "\n",
    "print(\"Dataframe shape after dropping NaN columns:\", discounted_df.shape)\n",
    "\n",
    "# Step 1: Separate features and target (if applicable)\n",
    "# Exclude non-numeric columns if present\n",
    "filtered_df = discounted_df.select_dtypes(include=[np.number])\n",
    "# columns_to_keep = [col for col in numeric_df.columns if 'RH' not in col]\n",
    "# filtered_df = filtered_df[columns_to_keep]\n",
    "\n",
    "print(f'Non-numeric columns removed from dataframe\\nCleaned dataframe size: {filtered_df.shape}')\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97056d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2, OPTION 1: Standardize the data using StandardScalar (for demo only)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaled_data = scaler.fit_transform(filtered_df)\n",
    "# print(f'Data scaled\\nScaled data size (ndarray): {scaled_data.shape}')\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=filtered_df.columns, index=filtered_df.index)\n",
    "print(\"Made new scaled dataframe\")\n",
    "scaled_df\n",
    "scaled_df.describe()\n",
    "\n",
    "# Uncomment the two lines below to write scaled_df to parquet\n",
    "scaled_df.to_parquet(\"input_standard_scaled.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "print('Successfully wrote to a .parquet file!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a602a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: Standardize the data using RobustScalar (this is better for outlier detection)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Assume `filtered_df` is your cleaned and filtered dataframe (all numeric columns)\n",
    "robust_scaler = RobustScaler()\n",
    "print(\"Instantiated RobustScalar\")\n",
    "\n",
    "# Fit the scaler on the dataframe and transform it\n",
    "scaled_array = robust_scaler.fit_transform(filtered_df)\n",
    "print(\"Fitted RobustScalar\")\n",
    "\n",
    "# (Optional) Create a new DataFrame with scaled values\n",
    "scaled_df = pd.DataFrame(scaled_array, columns=filtered_df.columns, index=filtered_df.index)\n",
    "print(\"Made new scaled dataframe\")\n",
    "scaled_df\n",
    "scaled_df.describe()\n",
    "\n",
    "# Uncomment the two lines below to write scaled_df to parquet\n",
    "scaled_df.to_parquet(\"input_scaled.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "print('Successfully wrote to a .parquet file!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEDI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
