{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43779e86e13e3b84",
   "metadata": {},
   "source": [
    "# Matching shot_number between GEDI L1B & ALS\n",
    "We will match shot numbers in the Level 2A GEDI data with the ALS crossover data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:12.404923Z",
     "start_time": "2024-12-03T16:42:12.088600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "# import geoviews as gv\n",
    "# from geoviews import opts, tile_sources as gvts\n",
    "# import holoviews as hv\n",
    "# gv.extension('bokeh', 'matplotlib')\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# import PyQt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abfb1225ff906d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:12.916873Z",
     "start_time": "2024-12-03T16:42:12.903957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/oscar/scratch/jzhu118/GEDI_Outlier_Detection_OSCAR/Input_files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['GEDI01_B_2022004042652_O17343_04_T10772_02_005_02_V002.h5',\n",
       " 'GEDI01_B_2022207041426_O20491_04_T09293_02_005_03_V002.h5']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inDir = os.getcwd() + \"/Input_files\"\n",
    "print(inDir)\n",
    "input_file_names = [g for g in os.listdir(inDir) if g.startswith('GEDI01_B') and g.endswith('.h5')]  # List all GEDI level 2 files in inDir\n",
    "input_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193cf3a589257efc",
   "metadata": {},
   "source": [
    "### Loading files with all information into a huge Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ca866",
   "metadata": {},
   "source": [
    "#### Preprocessing: Get files and sort by beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8a37290db29772d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:13.026872Z",
     "start_time": "2024-12-03T16:42:12.947179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: GEDI01_B_2022004042652_O17343_04_T10772_02_005_02_V002.h5\n",
      "The file contains the following groups: ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011', 'METADATA']\n",
      "The file's metadata contains the following attributes: \n",
      "PGEVersion\n",
      "VersionID\n",
      "abstract\n",
      "characterSet\n",
      "creationDate\n",
      "credit\n",
      "fileName\n",
      "language\n",
      "originatorOrganizationName\n",
      "purpose\n",
      "shortName\n",
      "spatialRepresentationType\n",
      "status\n",
      "topicCategory\n",
      "uuid\n",
      "The file contains the following beams: \n",
      "BEAM0000 is a Coverage beam\n",
      "BEAM0001 is a Coverage beam\n",
      "BEAM0010 is a Coverage beam\n",
      "BEAM0011 is a Coverage beam\n",
      "BEAM0101 is a Full power beam\n",
      "BEAM0110 is a Full power beam\n",
      "BEAM1000 is a Full power beam\n",
      "BEAM1011 is a Full power beam\n",
      "Loading file: GEDI01_B_2022207041426_O20491_04_T09293_02_005_03_V002.h5\n",
      "The file contains the following groups: ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011', 'METADATA']\n",
      "The file's metadata contains the following attributes: \n",
      "PGEVersion\n",
      "VersionID\n",
      "abstract\n",
      "characterSet\n",
      "creationDate\n",
      "credit\n",
      "fileName\n",
      "language\n",
      "originatorOrganizationName\n",
      "purpose\n",
      "shortName\n",
      "spatialRepresentationType\n",
      "status\n",
      "topicCategory\n",
      "uuid\n",
      "The file contains the following beams: \n",
      "BEAM0000 is a Coverage beam\n",
      "BEAM0001 is a Coverage beam\n",
      "BEAM0010 is a Coverage beam\n",
      "BEAM0011 is a Coverage beam\n",
      "BEAM0101 is a Full power beam\n",
      "BEAM0110 is a Full power beam\n",
      "BEAM1000 is a Full power beam\n",
      "BEAM1011 is a Full power beam\n"
     ]
    }
   ],
   "source": [
    "input_files = []\n",
    "files_to_beams = dict()\n",
    "for n in input_file_names:\n",
    "    file_path = os.path.join(inDir, n)  # Select an example file\n",
    "    file = h5py.File(file_path, 'r')\n",
    "    input_files.append(file)\n",
    "    \n",
    "    print('Loading file: ' + n)\n",
    "    print('The file contains the following groups: ' + str(list(file.keys())))\n",
    "    \n",
    "    print(\"The file's metadata contains the following attributes: \")\n",
    "    for g in file['METADATA']['DatasetIdentification'].attrs: print(g)\n",
    "    \n",
    "    beamNames = [g for g in file.keys() if g.startswith('BEAM')]\n",
    "    files_to_beams[file] = beamNames\n",
    "    \n",
    "    print(\"The file contains the following beams: \")\n",
    "    for b in beamNames:\n",
    "        print(f\"{b} is a {file[b].attrs['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14096b",
   "metadata": {},
   "source": [
    "#### Helper functions for the main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "660146583f485f77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:42:14.006557Z",
     "start_time": "2024-12-03T16:42:13.992165Z"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def collect_all_datasets(file_path):\n",
    "    \"\"\"\n",
    "    Recursively collects all dataset paths within the HDF5 file.\n",
    "    Returns a list of dataset path strings.\n",
    "    \"\"\"\n",
    "    dataset_paths = []\n",
    "    \n",
    "    def visitor_func(name, node):\n",
    "        if isinstance(node, h5py.Dataset):\n",
    "            dataset_paths.append(name)\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        f.visititems(visitor_func)\n",
    "    \n",
    "    return dataset_paths\n",
    "\n",
    "def get_dataset_by_name(h5_file, beam_ds, name):\n",
    "    \"\"\"\n",
    "    Given an open HDF5 file (h5_file) and a list of dataset paths (beam_ds),\n",
    "    returns the data for the first dataset whose path ends with '/{name}'.\n",
    "    If not found, returns None.\n",
    "    \"\"\"\n",
    "    candidates = [ds for ds in beam_ds if ds.endswith(f'/{name}')]\n",
    "    if not candidates:\n",
    "        print(f\"Warning: No dataset ending with '/{name}' found.\")\n",
    "        return None\n",
    "    dataset_path = candidates[0]\n",
    "    return h5_file[dataset_path][()]  # Read dataset into memory\n",
    "\n",
    "def load_attributes_to_dict(txt_file_path):\n",
    "    \"\"\"\n",
    "    Reads attribute names from a text file (one per line) and returns a dictionary.\n",
    "    This dictionary maps the human-friendly column name to the dataset suffix.\n",
    "    \"\"\"\n",
    "    with open(txt_file_path, 'r') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    attr_dict = {}\n",
    "    for raw_attr in lines:\n",
    "        attr_dict[raw_attr] = raw_attr  # Direct mapping for simplicity\n",
    "    return attr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a951e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ALS crossover data with 76778 unique shot numbers.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# LOAD ALS CROSSOVER DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "als_csv_path = os.path.join('Input_files', 'GEDI_ALSCROSSOVERS.csv')\n",
    "als_df = pd.read_csv(als_csv_path)\n",
    "\n",
    "# Ensure shot numbers are of a consistent type (adjust if necessary)\n",
    "als_df['shot_number'] = als_df['shot_number'].astype(str)\n",
    "als_shot_set = set(als_df['shot_number'])\n",
    "print(f\"Loaded ALS crossover data with {len(als_shot_set)} unique shot numbers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b7aa9",
   "metadata": {},
   "source": [
    "### THIS CELL IS FOR TESTING ONLY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5e948bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 182065 shot numbers for als_shot_set.\n"
     ]
    }
   ],
   "source": [
    "# Read the parquet file containing all GEDI waveform shots\n",
    "gedi_df = pd.read_parquet('L1B_raw.parquet')\n",
    "\n",
    "# Assume the shot numbers are in a column named 'shot_number'\n",
    "# Randomly sample 10% of the shot numbers for testing purposes.\n",
    "# Set a random state for reproducibility.\n",
    "sample_fraction = 0.1\n",
    "random_state = 42\n",
    "\n",
    "als_shot_series = gedi_df['Shot Number'].sample(frac=sample_fraction, random_state=random_state)\n",
    "\n",
    "# Convert shot numbers to strings for consistent matching later on\n",
    "als_shot_set = set(als_shot_series.astype(str))\n",
    "\n",
    "print(f\"Selected {len(als_shot_set)} shot numbers for als_shot_set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36d7a3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded attributes: {'all_samples_sum': 'all_samples_sum', 'beam': 'beam', 'channel': 'channel', 'master_frac': 'master_frac', 'master_int': 'master_int', 'noise_mean_corrected': 'noise_mean_corrected', 'noise_stddev_corrected': 'noise_stddev_corrected', 'nsemean_even': 'nsemean_even', 'nsemean_odd': 'nsemean_odd', 'rx_energy': 'rx_energy', 'rx_offset': 'rx_offset', 'rx_open': 'rx_open', 'rx_sample_count': 'rx_sample_count', 'rx_sample_start_index': 'rx_sample_start_index', 'selection_stretchers_x': 'selection_stretchers_x', 'selection_stretchers_y': 'selection_stretchers_y', 'shot_number': 'shot_number', 'stale_return_flag': 'stale_return_flag', 'th_left_used': 'th_left_used', 'tx_egamplitude': 'tx_egamplitude', 'tx_egamplitude_error': 'tx_egamplitude_error', 'tx_egbias': 'tx_egbias', 'tx_egbias_error': 'tx_egbias_error', 'tx_egflag': 'tx_egflag', 'tx_eggamma': 'tx_eggamma', 'tx_eggamma_error': 'tx_eggamma_error', 'tx_egsigma': 'tx_egsigma', 'tx_egsigma_error': 'tx_egsigma_error', 'tx_gloc': 'tx_gloc', 'tx_gloc_error': 'tx_gloc_error', 'tx_pulseflag': 'tx_pulseflag', 'tx_sample_count': 'tx_sample_count', 'tx_sample_start_index': 'tx_sample_start_index', 'altitude_instrument': 'altitude_instrument', 'altitude_instrument_error': 'altitude_instrument_error', 'bounce_time_offset_bin0': 'bounce_time_offset_bin0', 'bounce_time_offset_bin0_error': 'bounce_time_offset_bin0_error', 'bounce_time_offset_lastbin': 'bounce_time_offset_lastbin', 'bounce_time_offset_lastbin_error': 'bounce_time_offset_lastbin_error', 'degrade': 'degrade', 'delta_time': 'delta_time', 'digital_elevation_model': 'digital_elevation_model', 'elevation_bin0': 'elevation_bin0', 'elevation_bin0_error': 'elevation_bin0_error', 'elevation_lastbin': 'elevation_lastbin', 'elevation_lastbin_error': 'elevation_lastbin_error', 'latitude_bin0': 'latitude_bin0', 'latitude_bin0_error': 'latitude_bin0_error', 'latitude_lastbin': 'latitude_lastbin', 'latitude_lastbin_error': 'latitude_lastbin_error', 'latitude_instrument': 'latitude_instrument', 'latitude_instrument_error': 'latitude_instrument_error', 'local_beam_azimuth': 'local_beam_azimuth', 'local_beam_azimuth_error': 'local_beam_azimuth_error', 'local_beam_elevation': 'local_beam_elevation', 'local_beam_elevation_error': 'local_beam_elevation_error', 'longitude_bin0': 'longitude_bin0', 'longitude_bin0_error': 'longitude_bin0_error', 'longitude_lastbin': 'longitude_lastbin', 'longitude_lastbin_error': 'longitude_lastbin_error', 'longitude_instrument': 'longitude_instrument', 'longitude_instrument_error': 'longitude_instrument_error', 'mean_sea_surface': 'mean_sea_surface', 'neutat_delay_derivative_bin0': 'neutat_delay_derivative_bin0', 'neutat_delay_derivative_lastbin': 'neutat_delay_derivative_lastbin', 'neutat_delay_total_bin0': 'neutat_delay_total_bin0', 'neutat_delay_total_lastbin': 'neutat_delay_total_lastbin', 'range_bias_correction': 'range_bias_correction', 'solar_azimuth': 'solar_azimuth', 'solar_elevation': 'solar_elevation', 'dynamic_atmosphere_correction': 'dynamic_atmosphere_correction', 'geoid': 'geoid', 'tide_earth': 'tide_earth', 'tide_load': 'tide_load', 'tide_ocean': 'tide_ocean', 'tide_ocean_pole': 'tide_ocean_pole', 'tide_pole': 'tide_pole'}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# LOAD ATTRIBUTE DICTIONARY (including Beam and Channel, among others)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "txt_path = 'L1B_features.txt'\n",
    "ATTRIBUTES_TO_LOAD = load_attributes_to_dict(txt_path)\n",
    "print(\"Loaded attributes:\", ATTRIBUTES_TO_LOAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4574d5",
   "metadata": {},
   "source": [
    "#### Main loop, collects all target features and puts them inside a Pandas dataframe.\n",
    "WARNING: Despite my best attempts at optimization, this is *extremely* memory intensive even for one 5GB HDF5 file. Only run on a supercomputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbe575445c09a1d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:50:27.138768Z",
     "start_time": "2024-12-03T17:37:18.265405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: /oscar/scratch/jzhu118/GEDI_Outlier_Detection_OSCAR/Input_files/GEDI01_B_2022004042652_O17343_04_T10772_02_005_02_V002.h5\n",
      "  Processing beam: BEAM0000\n",
      "    Found 16820 matching shot(s).\n",
      "    Processed beam BEAM0000 with 16820 matching shots.\n",
      "  Processing beam: BEAM0001\n",
      "    Found 16647 matching shot(s).\n",
      "    Processed beam BEAM0001 with 16647 matching shots.\n",
      "  Processing beam: BEAM0010\n",
      "    Found 16738 matching shot(s).\n",
      "    Processed beam BEAM0010 with 16738 matching shots.\n",
      "  Processing beam: BEAM0011\n",
      "    Found 17068 matching shot(s).\n",
      "    Processed beam BEAM0011 with 17068 matching shots.\n",
      "  Processing beam: BEAM0101\n",
      "    Found 16810 matching shot(s).\n",
      "    Processed beam BEAM0101 with 16810 matching shots.\n",
      "  Processing beam: BEAM0110\n",
      "    Found 16803 matching shot(s).\n",
      "    Processed beam BEAM0110 with 16803 matching shots.\n",
      "  Processing beam: BEAM1000\n",
      "    Found 16898 matching shot(s).\n",
      "    Processed beam BEAM1000 with 16898 matching shots.\n",
      "  Processing beam: BEAM1011\n",
      "    Found 16991 matching shot(s).\n",
      "    Processed beam BEAM1011 with 16991 matching shots.\n",
      "\n",
      "Processing file: /oscar/scratch/jzhu118/GEDI_Outlier_Detection_OSCAR/Input_files/GEDI01_B_2022207041426_O20491_04_T09293_02_005_03_V002.h5\n",
      "  Processing beam: BEAM0000\n",
      "    Found 6071 matching shot(s).\n",
      "    Processed beam BEAM0000 with 6071 matching shots.\n",
      "  Processing beam: BEAM0001\n",
      "    Found 5881 matching shot(s).\n",
      "    Processed beam BEAM0001 with 5881 matching shots.\n",
      "  Processing beam: BEAM0010\n",
      "    Found 5876 matching shot(s).\n",
      "    Processed beam BEAM0010 with 5876 matching shots.\n",
      "  Processing beam: BEAM0011\n",
      "    Found 5764 matching shot(s).\n",
      "    Processed beam BEAM0011 with 5764 matching shots.\n",
      "  Processing beam: BEAM0101\n",
      "    Found 5898 matching shot(s).\n",
      "    Processed beam BEAM0101 with 5898 matching shots.\n",
      "  Processing beam: BEAM0110\n",
      "    Found 5984 matching shot(s).\n",
      "    Processed beam BEAM0110 with 5984 matching shots.\n",
      "  Processing beam: BEAM1000\n",
      "    Found 6019 matching shot(s).\n",
      "    Processed beam BEAM1000 with 6019 matching shots.\n",
      "  Processing beam: BEAM1011\n",
      "    Found 5797 matching shot(s).\n",
      "    Processed beam BEAM1011 with 5797 matching shots.\n",
      "ALL DONE!\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# PROCESS GEDI LEVEL 1B FILES & MATCH SHOT NUMBERS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for f in input_files:\n",
    "    print(f\"\\nProcessing file: {f.filename}\")\n",
    "    \n",
    "    # Collect all dataset paths in the current file\n",
    "    all_ds_for_file = collect_all_datasets(f.filename)\n",
    "    \n",
    "    # Process each beam in the current file\n",
    "    for b in files_to_beams[f]:\n",
    "        print(f\"  Processing beam: {b}\")\n",
    "        \n",
    "        # Filter dataset paths to only those belonging to the current beam\n",
    "        beam_ds = [ds for ds in all_ds_for_file if b in ds]\n",
    "        \n",
    "        # 1) Retrieve shot numbers for the beam and convert to string for matching\n",
    "        shotNums = get_dataset_by_name(f, beam_ds, 'shot_number')\n",
    "        if shotNums is None:\n",
    "            print(f\"    Warning: No 'shot_number' dataset found for beam {b}. Skipping beam.\")\n",
    "            continue\n",
    "        shotNums = np.array(shotNums).astype(str)\n",
    "        total_shots = len(shotNums)\n",
    "        \n",
    "        # 2) Identify indices where GEDI shot numbers match the ALS crossover shot numbers\n",
    "        matching_indices = np.where(np.isin(shotNums, list(als_shot_set)))[0]\n",
    "        if matching_indices.size == 0:\n",
    "            print(f\"    No matching shot numbers found for beam {b}.\")\n",
    "            continue\n",
    "        row_count = matching_indices.size\n",
    "        print(f\"    Found {row_count} matching shot(s).\")\n",
    "        \n",
    "        # Build constant columns: File Name and Beam Name\n",
    "        file_name_col = [os.path.basename(f.filename)] * row_count\n",
    "        beam_name_col = [b] * row_count\n",
    "        \n",
    "        # 3) Extract all other attributes as specified in the txt file\n",
    "        attr_data = {}\n",
    "        for col_label, ds_suffix in ATTRIBUTES_TO_LOAD.items():\n",
    "            data_array = get_dataset_by_name(f, beam_ds, ds_suffix)\n",
    "            if data_array is None:\n",
    "                print(f\"    Warning: Missing dataset for {ds_suffix}. Filling with None.\")\n",
    "                attr_data[col_label] = np.array([None] * total_shots)[matching_indices]\n",
    "            else:\n",
    "                data_array = np.array(data_array)\n",
    "                if data_array.shape[0] != shotNums.shape[0]:\n",
    "                    print(f\"    Warning: Row count mismatch for {col_label} in beam {b}.\")\n",
    "                attr_data[col_label] = data_array[matching_indices]\n",
    "        \n",
    "        # 4) Extract the 'surface_type' feature (2D array with shape (5, # of shots))\n",
    "        surface_data = get_dataset_by_name(f, beam_ds, 'surface_type')\n",
    "        if surface_data is None:\n",
    "            # Handle missing surface_type as before\n",
    "            surface_cols = {\n",
    "                'surface_land': np.full(total_shots, np.nan)[matching_indices],\n",
    "                'surface_ocean': np.full(total_shots, np.nan)[matching_indices],\n",
    "                'surface_sea_ice': np.full(total_shots, np.nan)[matching_indices],\n",
    "                'surface_land_ice': np.full(total_shots, np.nan)[matching_indices],\n",
    "                'surface_inland_water': np.full(total_shots, np.nan)[matching_indices]\n",
    "            }\n",
    "        else:\n",
    "            surface_data = np.array(surface_data)\n",
    "            # Transpose if necessary to get shape (total_shots, 5)\n",
    "            if surface_data.shape[0] == 5 and surface_data.shape[1] == total_shots:\n",
    "                surface_data = surface_data.T\n",
    "            elif surface_data.shape[0] != total_shots or surface_data.shape[1] != 5:\n",
    "                print(\"    Warning: Unexpected shape for 'surface_type'. Expected (5, #shots) or (#shots, 5).\")\n",
    "            # Now filter each column using matching_indices\n",
    "            surface_cols = {\n",
    "                'surface_land': surface_data[:, 0][matching_indices],\n",
    "                'surface_ocean': surface_data[:, 1][matching_indices],\n",
    "                'surface_sea_ice': surface_data[:, 2][matching_indices],\n",
    "                'surface_land_ice': surface_data[:, 3][matching_indices],\n",
    "                'surface_inland_water': surface_data[:, 4][matching_indices]\n",
    "            }\n",
    "        \n",
    "        # 5) Filter the shot numbers to only the matching ones\n",
    "        shotNums_filtered = shotNums[matching_indices]\n",
    "        \n",
    "        # 6) Construct the DataFrame for the current beam\n",
    "        df = pd.DataFrame({\n",
    "            'File Name': file_name_col,\n",
    "            'Beam Name': beam_name_col,\n",
    "            'Shot Number': shotNums_filtered,\n",
    "            **attr_data,\n",
    "            **surface_cols\n",
    "        })\n",
    "        \n",
    "        # 7) One-hot encode the Beam and Channel identifiers.\n",
    "        #    We assume that both 'Beam Name' and 'Channel' columns exist.\n",
    "        if 'Beam Name' in df.columns:\n",
    "            df = pd.get_dummies(df, columns=['Beam Name'], prefix='')\n",
    "        if 'channel' in df.columns:\n",
    "            df = pd.get_dummies(df, columns=['channel'], prefix='Channel')\n",
    "        \n",
    "        dataframes.append(df)\n",
    "        print(f\"    Processed beam {b} with {row_count} matching shots.\")\n",
    "\n",
    "print('ALL DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbdb9ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame shape: (182065, 97)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# FINALIZE: Concatenate all partial DataFrames into one final DataFrame\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if dataframes:\n",
    "    complete_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\nFinal DataFrame shape: {complete_df.shape}\")\n",
    "else:\n",
    "    complete_df = pd.DataFrame()\n",
    "    print(\"\\nNo matching data found in any beams.\")\n",
    "    \n",
    "# Optionally, save the complete DataFrame for further processing:\n",
    "# complete_df.to_parquet(\"ALS_and_L1B.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "# print('Successfully wrote to a .parquet file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73cf29",
   "metadata": {},
   "source": [
    "#### Feature engineering\n",
    "1. Adding Z-Scores for five most relevant RH metrics\n",
    "2. Creating RH50 / RH100 ratio\n",
    "3. Adding RH95 - RH50\n",
    "4. Adding \"Missingness\" - the number of NaNs in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zeros = (complete_df['RH_100'] == 0).sum()\n",
    "print(f\"Number of zeros in RH_100: {num_zeros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ed4109accefc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T15:26:48.589282Z",
     "start_time": "2024-10-28T15:26:48.291657Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Add Z-Scores of the five RH metrics'''\n",
    "rh_nums = [25, 50, 75, 85, 95, 100]\n",
    "for i in rh_nums:\n",
    "    col_name = f'RH_{i}'\n",
    "    complete_df[f'{col_name} Z Score'] = (complete_df[col_name] - complete_df[col_name].mean()) / complete_df[col_name].std()\n",
    "\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a756676",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding the RH_50 / RH_100 feature'''\n",
    "complete_df['RH_50_v_100'] = complete_df['RH_50'] / complete_df['RH_100']\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding the (RH95 - RH50) feature'''\n",
    "rh_50 = complete_df['RH_50']\n",
    "rh_95 = complete_df['RH_95']\n",
    "complete_df['RH_95_minus_50'] = (rh_95 - rh_50)\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding the 'Missingness' feature'''\n",
    "# Count the number of NaNs in each row\n",
    "complete_df['Missingness'] = complete_df.isna().sum(axis=1)\n",
    "\n",
    "\n",
    "# Optionally, inspect how many rows have missing data\n",
    "print('Number of NaNs:')\n",
    "print(complete_df['Missingness'].value_counts())\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1187d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Optional: Save to a parquet file'''\n",
    "# Get a list of column names\n",
    "columns = complete_df.columns\n",
    "\n",
    "# Find duplicates\n",
    "duplicate_columns = columns[columns.duplicated()]\n",
    "print(\"Duplicate columns:\", duplicate_columns)\n",
    "print('There are ' + str(columns.duplicated().sum()) + ' duplicated columns.')\n",
    "print('NaN value count:\\n')\n",
    "print(complete_df.isnull().sum())\n",
    "# complete_df.describe()\n",
    "\n",
    "# Uncomment the two lines below to write to parquet\n",
    "# complete_df.to_parquet(\"input_raw.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "# print('Successfully wrote to a .parquet file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307ec82",
   "metadata": {},
   "source": [
    "#### This is the filtering step for PCA if there are any NaN rows in the dataframe\n",
    "Note: there should not be mnany NaN values if you accounted for the different shapes of the data inside the HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Optional: load complete_df from .parquet file'''\n",
    "complete_df = pd.read_parquet('input_raw.parquet', engine='pyarrow')\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e344202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(f'Original dataframe shape: {complete_df.shape}')\n",
    "\n",
    "'''Filtering'''\n",
    "complete_df.drop('Shot Number', axis=1, inplace=True)\n",
    "print(\"Shot numbers dropped\")\n",
    "discounted_df = complete_df.dropna(axis=1)\n",
    "\n",
    "\n",
    "# imputer = SimpleImputer(strategy='most_frequent')  # You can change to 'median', 'most_frequent', etc.\n",
    "# discounted_df = pd.DataFrame(imputer.fit_transform(complete_df), columns=complete_df.columns)\n",
    "\n",
    "print(\"Dataframe shape after dropping NaN columns:\", discounted_df.shape)\n",
    "\n",
    "# Step 1: Separate features and target (if applicable)\n",
    "# Exclude non-numeric columns if present\n",
    "filtered_df = discounted_df.select_dtypes(include=[np.number])\n",
    "# columns_to_keep = [col for col in numeric_df.columns if 'RH' not in col]\n",
    "# filtered_df = filtered_df[columns_to_keep]\n",
    "\n",
    "print(f'Non-numeric columns removed from dataframe\\nCleaned dataframe size: {filtered_df.shape}')\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97056d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2, OPTION 1: Standardize the data using StandardScalar (for demo only)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaled_data = scaler.fit_transform(filtered_df)\n",
    "# print(f'Data scaled\\nScaled data size (ndarray): {scaled_data.shape}')\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=filtered_df.columns, index=filtered_df.index)\n",
    "print(\"Made new scaled dataframe\")\n",
    "scaled_df\n",
    "scaled_df.describe()\n",
    "\n",
    "# Uncomment the two lines below to write scaled_df to parquet\n",
    "scaled_df.to_parquet(\"input_standard_scaled.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "print('Successfully wrote to a .parquet file!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a602a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: Standardize the data using RobustScalar (this is better for outlier detection)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Assume `filtered_df` is your cleaned and filtered dataframe (all numeric columns)\n",
    "robust_scaler = RobustScaler()\n",
    "print(\"Instantiated RobustScalar\")\n",
    "\n",
    "# Fit the scaler on the dataframe and transform it\n",
    "scaled_array = robust_scaler.fit_transform(filtered_df)\n",
    "print(\"Fitted RobustScalar\")\n",
    "\n",
    "# (Optional) Create a new DataFrame with scaled values\n",
    "scaled_df = pd.DataFrame(scaled_array, columns=filtered_df.columns, index=filtered_df.index)\n",
    "print(\"Made new scaled dataframe\")\n",
    "scaled_df\n",
    "scaled_df.describe()\n",
    "\n",
    "# Uncomment the two lines below to write scaled_df to parquet\n",
    "scaled_df.to_parquet(\"input_scaled.parquet\", engine=\"pyarrow\", compression=\"snappy\")\n",
    "print('Successfully wrote to a .parquet file!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEDI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
